{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Ruia Overview An async web scraping micro-framework, written with asyncio and aiohttp , aims to make crawling url as convenient as possible. Write less, run faster: Documentation: \u4e2d\u6587\u6587\u6863 | documentation Awesome: https://github.com/ruia-plugins/awesome-ruia Organization: https://github.com/ruia-plugins Features Easy : Declarative programming Fast : Powered by asyncio Extensible : Middlewares and plugins Powerful : JavaScript support Installation # For Linux & Mac pip install -U ruia [ uvloop ] # For Windows pip install -U ruia # New features pip install git+https://github.com/howie6879/ruia Tutorials Overview Installation Define Data Items Spider Control Request & Response Customize Middleware Write a Plugins Usage Item Item can be used standalone, for testing, and for tiny crawlers, Create a file named item_demo.py import asyncio from ruia import AttrField , TextField , Item class HackerNewsItem ( Item ): target_item = TextField ( css_select = 'tr.athing' ) title = TextField ( css_select = 'a.storylink' ) url = AttrField ( css_select = 'a.storylink' , attr = 'href' ) async def main (): async for item in HackerNewsItem . get_items ( url = \"https://news.ycombinator.com/\" ): print ( item . title , item . url ) if __name__ == '__main__' : items = asyncio . run ( main ()) Run: python item_demo.py Notorious \u2018Hijack Factory\u2019 Shunned from Web https://krebsonsecurity.com/2018/07/notorious-hijack-factory-shunned-from-web/ ...... Spider Control Spider is used for control requests better. from ruia import AttrField , TextField , Item , Spider class HackerNewsItem ( Item ): target_item = TextField ( css_select = 'tr.athing' ) title = TextField ( css_select = 'a.storylink' ) url = AttrField ( css_select = 'a.storylink' , attr = 'href' ) async def clean_title ( self , value ): \"\"\"Define clean_* functions for data cleaning\"\"\" return value . strip () class HackerNewsSpider ( Spider ): start_urls = [ f 'https://news.ycombinator.com/news?p={index}' for index in range ( 1 , 3 )] concurrency = 10 async def parse ( self , response ): async for item in HackerNewsItem . get_items ( html = response . html ): yield item if __name__ == '__main__' : HackerNewsSpider . start () More details click here Run hacker_news_spider.py : [ 2018 -09-21 17 :27:14,497 ] -ruia-INFO spider::l54: Spider started! [ 2018 -09-21 17 :27:14,502 ] -Request-INFO request::l77: <GET: https://news.ycombinator.com/news?p = 2 > [ 2018 -09-21 17 :27:14,527 ] -Request-INFO request::l77: <GET: https://news.ycombinator.com/news?p = 1 > [ 2018 -09-21 17 :27:16,388 ] -ruia-INFO spider::l122: Stopping spider: ruia [ 2018 -09-21 17 :27:16,389 ] -ruia-INFO spider::l68: Total requests: 2 [ 2018 -09-21 17 :27:16,389 ] -ruia-INFO spider::l71: Time usage: 0 :00:01.891688 [ 2018 -09-21 17 :27:16,389 ] -ruia-INFO spider::l72: Spider finished! Custom middleware ruia provides an easy way to customize requests. The following middleware is based on the above example: from ruia import Middleware middleware = Middleware () @middleware.request async def print_on_request ( request ): request . metadata = { 'url' : request . url } print ( f \"request: {request.metadata}\" ) # Just operate request object, and do not return anything. @middleware.response async def print_on_response ( request , response ): print ( f \"response: {response.metadata}\" ) # Add your spider here More details click here JavaScript Support You can load js by using ruia-pyppeteer . For example: import asyncio from ruia_pyppeteer import PyppeteerRequest as Request request = Request ( \"https://www.jianshu.com/\" , load_js = True ) response = asyncio . run ( request . fetch ()) # Python 3.7 print ( response . html ) TODO Cache for debug, to decreasing request limitation Distributed crawling/scraping Contribution Ruia is still under developing, feel free to open issues and pull requests: Report or fix bugs Require or publish plugins Write or fix documentation Add test cases Thanks aiohttp demiurge","title":"Introduction"},{"location":"index.html#ruia","text":"","title":"Ruia"},{"location":"index.html#overview","text":"An async web scraping micro-framework, written with asyncio and aiohttp , aims to make crawling url as convenient as possible. Write less, run faster: Documentation: \u4e2d\u6587\u6587\u6863 | documentation Awesome: https://github.com/ruia-plugins/awesome-ruia Organization: https://github.com/ruia-plugins","title":"Overview"},{"location":"index.html#features","text":"Easy : Declarative programming Fast : Powered by asyncio Extensible : Middlewares and plugins Powerful : JavaScript support","title":"Features"},{"location":"index.html#installation","text":"# For Linux & Mac pip install -U ruia [ uvloop ] # For Windows pip install -U ruia # New features pip install git+https://github.com/howie6879/ruia","title":"Installation"},{"location":"index.html#tutorials","text":"Overview Installation Define Data Items Spider Control Request & Response Customize Middleware Write a Plugins","title":"Tutorials"},{"location":"index.html#usage","text":"","title":"Usage"},{"location":"index.html#item","text":"Item can be used standalone, for testing, and for tiny crawlers, Create a file named item_demo.py import asyncio from ruia import AttrField , TextField , Item class HackerNewsItem ( Item ): target_item = TextField ( css_select = 'tr.athing' ) title = TextField ( css_select = 'a.storylink' ) url = AttrField ( css_select = 'a.storylink' , attr = 'href' ) async def main (): async for item in HackerNewsItem . get_items ( url = \"https://news.ycombinator.com/\" ): print ( item . title , item . url ) if __name__ == '__main__' : items = asyncio . run ( main ()) Run: python item_demo.py Notorious \u2018Hijack Factory\u2019 Shunned from Web https://krebsonsecurity.com/2018/07/notorious-hijack-factory-shunned-from-web/ ......","title":"Item"},{"location":"index.html#spider-control","text":"Spider is used for control requests better. from ruia import AttrField , TextField , Item , Spider class HackerNewsItem ( Item ): target_item = TextField ( css_select = 'tr.athing' ) title = TextField ( css_select = 'a.storylink' ) url = AttrField ( css_select = 'a.storylink' , attr = 'href' ) async def clean_title ( self , value ): \"\"\"Define clean_* functions for data cleaning\"\"\" return value . strip () class HackerNewsSpider ( Spider ): start_urls = [ f 'https://news.ycombinator.com/news?p={index}' for index in range ( 1 , 3 )] concurrency = 10 async def parse ( self , response ): async for item in HackerNewsItem . get_items ( html = response . html ): yield item if __name__ == '__main__' : HackerNewsSpider . start () More details click here Run hacker_news_spider.py : [ 2018 -09-21 17 :27:14,497 ] -ruia-INFO spider::l54: Spider started! [ 2018 -09-21 17 :27:14,502 ] -Request-INFO request::l77: <GET: https://news.ycombinator.com/news?p = 2 > [ 2018 -09-21 17 :27:14,527 ] -Request-INFO request::l77: <GET: https://news.ycombinator.com/news?p = 1 > [ 2018 -09-21 17 :27:16,388 ] -ruia-INFO spider::l122: Stopping spider: ruia [ 2018 -09-21 17 :27:16,389 ] -ruia-INFO spider::l68: Total requests: 2 [ 2018 -09-21 17 :27:16,389 ] -ruia-INFO spider::l71: Time usage: 0 :00:01.891688 [ 2018 -09-21 17 :27:16,389 ] -ruia-INFO spider::l72: Spider finished!","title":"Spider Control"},{"location":"index.html#custom-middleware","text":"ruia provides an easy way to customize requests. The following middleware is based on the above example: from ruia import Middleware middleware = Middleware () @middleware.request async def print_on_request ( request ): request . metadata = { 'url' : request . url } print ( f \"request: {request.metadata}\" ) # Just operate request object, and do not return anything. @middleware.response async def print_on_response ( request , response ): print ( f \"response: {response.metadata}\" ) # Add your spider here More details click here","title":"Custom middleware"},{"location":"index.html#javascript-support","text":"You can load js by using ruia-pyppeteer . For example: import asyncio from ruia_pyppeteer import PyppeteerRequest as Request request = Request ( \"https://www.jianshu.com/\" , load_js = True ) response = asyncio . run ( request . fetch ()) # Python 3.7 print ( response . html )","title":"JavaScript Support"},{"location":"index.html#todo","text":"Cache for debug, to decreasing request limitation Distributed crawling/scraping","title":"TODO"},{"location":"index.html#contribution","text":"Ruia is still under developing, feel free to open issues and pull requests: Report or fix bugs Require or publish plugins Write or fix documentation Add test cases","title":"Contribution"},{"location":"index.html#thanks","text":"aiohttp demiurge","title":"Thanks"},{"location":"cn/README.html","text":"Ruia Ruia \u662f\u4e00\u4e2a\u57fa\u4e8e asyncio \u548c aiohttp \u7684\u5f02\u6b65\u722c\u866b\u6846\u67b6\uff0c\u5b83\u7684\u76ee\u6807\u662f\u8ba9\u4f60\u66f4\u52a0\u65b9\u4fbf\u4e14\u8fc5\u901f\u5730\u7f16\u5199\u51fa\u5c5e\u4e8e\u81ea\u5df1\u7684\u722c\u866b \u5f88\u9ad8\u5174\u4f60\u80fd\u4f7f\u7528 Ruia \u6765\u5b9e\u73b0\u722c\u866b\u7a0b\u5e8f\uff0c\u4e0d\u8fc7\u5728\u7f16\u7801\u4e4b\u524d\uff0c\u5e0c\u671b\u4f60\u80fd\u901a\u8bfb\u6b64\u6587\u6863\uff0c\u56e0\u4e3a\u5b83\u5305\u542b\u4e86 Ruia \u7684\u4f7f\u7528\u65b9\u6cd5\u4ee5\u53ca\u4e00\u4e9b\u57fa\u7840\u6982\u5ff5\u4ecb\u7ecd First steps Introduction \uff1a\u4ecb\u7ecdRuia Tutorials \uff1a\u4f7f\u7528Ruia\u5feb\u901f\u7f16\u5199\u4e00\u4e2a\u7a33\u5065\u7684\u722c\u866b Plugins \uff1a\u7f16\u5199Ruia\u6269\u5c55 Topics Item \uff1a\u5b9a\u4e49\u722c\u866b\u7684\u76ee\u6807\u5b57\u6bb5 Selector \uff1a\u4eceHTML\u4e2d\u63d0\u53d6\u51fa\u76ee\u6807\u5b57\u6bb5 Request \uff1a\u8bf7\u6c42\u5e76\u6293\u53d6\u76ee\u6807\u7f51\u7ad9\u8d44\u6e90 Response \uff1a\u8fdb\u4e00\u6b65\u5c01\u88c5\u54cd\u5e94\u5185\u5bb9 Middleware \uff1a\u4f7f\u722c\u866b\u652f\u6301\u7b2c\u4e09\u65b9\u6269\u5c55 Spider \uff1a\u722c\u866b\u7a0b\u5e8f\u7684\u5165\u53e3 Getting help \u5982\u679c\u5728\u4f7f\u7528\u8fc7\u7a0b\u4e2d\u9047\u5230\u4e86\u56f0\u96be\uff0c\u968f\u65f6\u6b22\u8fce\u63d0 Issue \u4e5f\u968f\u65f6\u6b22\u8fce\u52a0\u6211\u5fae\u4fe1\u62c9\u60a8\u8fdb\u7fa4\u4ea4\u6d41\uff0c\u5907\u6ce8(Ruia)\uff1a","title":"Ruia"},{"location":"cn/README.html#ruia","text":"Ruia \u662f\u4e00\u4e2a\u57fa\u4e8e asyncio \u548c aiohttp \u7684\u5f02\u6b65\u722c\u866b\u6846\u67b6\uff0c\u5b83\u7684\u76ee\u6807\u662f\u8ba9\u4f60\u66f4\u52a0\u65b9\u4fbf\u4e14\u8fc5\u901f\u5730\u7f16\u5199\u51fa\u5c5e\u4e8e\u81ea\u5df1\u7684\u722c\u866b \u5f88\u9ad8\u5174\u4f60\u80fd\u4f7f\u7528 Ruia \u6765\u5b9e\u73b0\u722c\u866b\u7a0b\u5e8f\uff0c\u4e0d\u8fc7\u5728\u7f16\u7801\u4e4b\u524d\uff0c\u5e0c\u671b\u4f60\u80fd\u901a\u8bfb\u6b64\u6587\u6863\uff0c\u56e0\u4e3a\u5b83\u5305\u542b\u4e86 Ruia \u7684\u4f7f\u7528\u65b9\u6cd5\u4ee5\u53ca\u4e00\u4e9b\u57fa\u7840\u6982\u5ff5\u4ecb\u7ecd","title":"Ruia"},{"location":"cn/README.html#first-steps","text":"Introduction \uff1a\u4ecb\u7ecdRuia Tutorials \uff1a\u4f7f\u7528Ruia\u5feb\u901f\u7f16\u5199\u4e00\u4e2a\u7a33\u5065\u7684\u722c\u866b Plugins \uff1a\u7f16\u5199Ruia\u6269\u5c55","title":"First steps"},{"location":"cn/README.html#topics","text":"Item \uff1a\u5b9a\u4e49\u722c\u866b\u7684\u76ee\u6807\u5b57\u6bb5 Selector \uff1a\u4eceHTML\u4e2d\u63d0\u53d6\u51fa\u76ee\u6807\u5b57\u6bb5 Request \uff1a\u8bf7\u6c42\u5e76\u6293\u53d6\u76ee\u6807\u7f51\u7ad9\u8d44\u6e90 Response \uff1a\u8fdb\u4e00\u6b65\u5c01\u88c5\u54cd\u5e94\u5185\u5bb9 Middleware \uff1a\u4f7f\u722c\u866b\u652f\u6301\u7b2c\u4e09\u65b9\u6269\u5c55 Spider \uff1a\u722c\u866b\u7a0b\u5e8f\u7684\u5165\u53e3","title":"Topics"},{"location":"cn/README.html#getting-help","text":"\u5982\u679c\u5728\u4f7f\u7528\u8fc7\u7a0b\u4e2d\u9047\u5230\u4e86\u56f0\u96be\uff0c\u968f\u65f6\u6b22\u8fce\u63d0 Issue \u4e5f\u968f\u65f6\u6b22\u8fce\u52a0\u6211\u5fae\u4fe1\u62c9\u60a8\u8fdb\u7fa4\u4ea4\u6d41\uff0c\u5907\u6ce8(Ruia)\uff1a","title":"Getting help"},{"location":"cn/introduction.html","text":"Introduction Ruia \u662f\u4e00\u4e2a\u57fa\u4e8e asyncio \u548c aiohttp \u7684\u5f02\u6b65\u722c\u866b\u6846\u67b6\uff0c\u5b83\u5177\u6709\u7f16\u5199\u5feb\u901f\uff0c\u975e\u963b\u585e\uff0c\u6269\u5c55\u6027\u5f3a\u7b49\u7279\u70b9\uff0c \u8ba9\u4f60\u5199\u66f4\u5c11\u7684\u4ee3\u7801\uff0c\u6536\u83b7\u66f4\u5feb\u7684\u8fd0\u884c\u901f\u5ea6\u3002 \u7279\u6027\u5982\u4e0b\uff1a - \u81ea\u5b9a\u4e49\u4e2d\u95f4\u4ef6 - \u652f\u6301js\u52a0\u8f7d\u7c7b\u578b\u7f51\u9875 - \u53cb\u597d\u5730\u6570\u636e\u54cd\u5e94\u7c7b - \u5f02\u6b65\u65e0\u963b\u585e Installation \u5b89\u88c5 Ruia \u4e4b\u524d\u8bf7\u5148\u786e\u4fdd\u4f60\u4f7f\u7528\u7684\u662f Python3.6+ # For Linux & Mac pip install -U ruia [ uvloop ] # For Windows pip install -U ruia # New features pip install git+https://github.com/howie6879/ruia Code Snippets \u4e0b\u9762\u6211\u5c06\u4e3e\u4e2a\u4f8b\u5b50\u7b80\u5355\u4ecb\u7ecd\u4e0b Ruia \u7684\u4f7f\u7528\u65b9\u5f0f\u4ee5\u53ca\u6846\u67b6\u8fd0\u884c\u6d41\u7a0b\uff0c\u521b\u5efa\u6587\u4ef6 hacker_news_spider.py \uff0c\u7136\u540e\u62f7\u8d1d\u4e0b\u9762\u4ee3\u7801\u5230\u6587\u4ef6\u4e2d\uff1a #!/usr/bin/env python \"\"\" Target: https://news.ycombinator.com/ pip install aiofiles \"\"\" import aiofiles from ruia import AttrField , TextField , Item , Spider class HackerNewsItem ( Item ): target_item = TextField ( css_select = 'tr.athing' ) title = TextField ( css_select = 'a.storylink' ) url = AttrField ( css_select = 'a.storylink' , attr = 'href' ) async def clean_title ( self , value ): \"\"\" \u5982\u679c\u5b57\u6bb5\u4e0d\u9700\u8981\u6e05\u6d17 \u8fd9\u4e2a\u51fd\u6570\u53ef\u4ee5\u4e0d\u5199 \"\"\" return value class HackerNewsSpider ( Spider ): start_urls = [ 'https://news.ycombinator.com/news?p=1' , 'https://news.ycombinator.com/news?p=2' ] concurrency = 10 async def parse ( self , response ): async for item in HackerNewsItem . get_items ( html = response . html ): yield item async def process_item ( self , item : HackerNewsItem ): \"\"\"Ruia build-in method\"\"\" async with aiofiles . open ( './hacker_news.txt' , 'a' ) as f : await f . write ( str ( item . title ) + ' \\n ' ) if __name__ == '__main__' : HackerNewsSpider . start ( middleware = None ) \u5728\u7ec8\u7aef\u6267\u884c python hacker_news_spider.py \uff0c\u5982\u679c\u987a\u5229\u7684\u8bdd\u5c06\u4f1a\u5f97\u5230\u5982\u4e0b\u8f93\u51fa\uff0c\u5e76\u4e14\u76ee\u6807\u6570\u636e\u4f1a\u5b58\u50a8\u5728 hacker_news.txt \u6587\u4ef6\u4e2d\uff1a [ 2018 -09-24 11 :02:05,088 ] -ruia-INFO spider : Spider started! [ 2018 -09-24 11 :02:05,089 ] -Request-INFO request: <GET: https://news.ycombinator.com/news?p = 2 > [ 2018 -09-24 11 :02:05,113 ] -Request-INFO request: <GET: https://news.ycombinator.com/news?p = 1 > [ 2018 -09-24 11 :02:09,820 ] -ruia-INFO spider : Stopping spider: ruia [ 2018 -09-24 11 :02:09,820 ] -ruia-INFO spider : Total requests: 2 [ 2018 -09-24 11 :02:09,820 ] -ruia-INFO spider : Time usage: 0 :00:01.731780 [ 2018 -09-24 11 :02:09,821 ] -ruia-INFO spider : Spider finished! Getting help \u5982\u679c\u7a0b\u5e8f\u8fd0\u884c\u5730\u4e0d\u591f\u987a\u5229\uff0c\u8bf7\u603b\u7ed3\u95ee\u9898\u65e5\u5fd7\uff0c\u5e76\u7ed9\u6211\u4eec\u63d0\u4ea4 Issue \u606d\u559c\u4f60\uff0c\u4f60\u5df2\u7ecf\u7f16\u5199\u4e86\u4e00\u4e2a\u5c5e\u4e8e\u81ea\u5df1\u7684\u5f02\u6b65\u722c\u866b\uff0c\u662f\u4e0d\u662f\u5f88\u7b80\u5355\uff0c\u63a5\u4e0b\u6765\u4f60\u5c06\u5b9e\u9645\u7f16\u5199\u4e00\u4e2a\u4f8b\u5b50\uff0c\u4f1a\u66f4\u52a0\u6df1\u523b\u5730\u8ba4\u8bc6\u5230 Ruia \u7684\u5f3a\u5927\u4e4b\u5904\uff0c\u8bf7\u7ee7\u7eed\u9605\u8bfb Tutorials","title":"Introduction"},{"location":"cn/introduction.html#introduction","text":"Ruia \u662f\u4e00\u4e2a\u57fa\u4e8e asyncio \u548c aiohttp \u7684\u5f02\u6b65\u722c\u866b\u6846\u67b6\uff0c\u5b83\u5177\u6709\u7f16\u5199\u5feb\u901f\uff0c\u975e\u963b\u585e\uff0c\u6269\u5c55\u6027\u5f3a\u7b49\u7279\u70b9\uff0c \u8ba9\u4f60\u5199\u66f4\u5c11\u7684\u4ee3\u7801\uff0c\u6536\u83b7\u66f4\u5feb\u7684\u8fd0\u884c\u901f\u5ea6\u3002 \u7279\u6027\u5982\u4e0b\uff1a - \u81ea\u5b9a\u4e49\u4e2d\u95f4\u4ef6 - \u652f\u6301js\u52a0\u8f7d\u7c7b\u578b\u7f51\u9875 - \u53cb\u597d\u5730\u6570\u636e\u54cd\u5e94\u7c7b - \u5f02\u6b65\u65e0\u963b\u585e","title":"Introduction"},{"location":"cn/introduction.html#installation","text":"\u5b89\u88c5 Ruia \u4e4b\u524d\u8bf7\u5148\u786e\u4fdd\u4f60\u4f7f\u7528\u7684\u662f Python3.6+ # For Linux & Mac pip install -U ruia [ uvloop ] # For Windows pip install -U ruia # New features pip install git+https://github.com/howie6879/ruia","title":"Installation"},{"location":"cn/introduction.html#code-snippets","text":"\u4e0b\u9762\u6211\u5c06\u4e3e\u4e2a\u4f8b\u5b50\u7b80\u5355\u4ecb\u7ecd\u4e0b Ruia \u7684\u4f7f\u7528\u65b9\u5f0f\u4ee5\u53ca\u6846\u67b6\u8fd0\u884c\u6d41\u7a0b\uff0c\u521b\u5efa\u6587\u4ef6 hacker_news_spider.py \uff0c\u7136\u540e\u62f7\u8d1d\u4e0b\u9762\u4ee3\u7801\u5230\u6587\u4ef6\u4e2d\uff1a #!/usr/bin/env python \"\"\" Target: https://news.ycombinator.com/ pip install aiofiles \"\"\" import aiofiles from ruia import AttrField , TextField , Item , Spider class HackerNewsItem ( Item ): target_item = TextField ( css_select = 'tr.athing' ) title = TextField ( css_select = 'a.storylink' ) url = AttrField ( css_select = 'a.storylink' , attr = 'href' ) async def clean_title ( self , value ): \"\"\" \u5982\u679c\u5b57\u6bb5\u4e0d\u9700\u8981\u6e05\u6d17 \u8fd9\u4e2a\u51fd\u6570\u53ef\u4ee5\u4e0d\u5199 \"\"\" return value class HackerNewsSpider ( Spider ): start_urls = [ 'https://news.ycombinator.com/news?p=1' , 'https://news.ycombinator.com/news?p=2' ] concurrency = 10 async def parse ( self , response ): async for item in HackerNewsItem . get_items ( html = response . html ): yield item async def process_item ( self , item : HackerNewsItem ): \"\"\"Ruia build-in method\"\"\" async with aiofiles . open ( './hacker_news.txt' , 'a' ) as f : await f . write ( str ( item . title ) + ' \\n ' ) if __name__ == '__main__' : HackerNewsSpider . start ( middleware = None ) \u5728\u7ec8\u7aef\u6267\u884c python hacker_news_spider.py \uff0c\u5982\u679c\u987a\u5229\u7684\u8bdd\u5c06\u4f1a\u5f97\u5230\u5982\u4e0b\u8f93\u51fa\uff0c\u5e76\u4e14\u76ee\u6807\u6570\u636e\u4f1a\u5b58\u50a8\u5728 hacker_news.txt \u6587\u4ef6\u4e2d\uff1a [ 2018 -09-24 11 :02:05,088 ] -ruia-INFO spider : Spider started! [ 2018 -09-24 11 :02:05,089 ] -Request-INFO request: <GET: https://news.ycombinator.com/news?p = 2 > [ 2018 -09-24 11 :02:05,113 ] -Request-INFO request: <GET: https://news.ycombinator.com/news?p = 1 > [ 2018 -09-24 11 :02:09,820 ] -ruia-INFO spider : Stopping spider: ruia [ 2018 -09-24 11 :02:09,820 ] -ruia-INFO spider : Total requests: 2 [ 2018 -09-24 11 :02:09,820 ] -ruia-INFO spider : Time usage: 0 :00:01.731780 [ 2018 -09-24 11 :02:09,821 ] -ruia-INFO spider : Spider finished!","title":"Code Snippets"},{"location":"cn/introduction.html#getting-help","text":"\u5982\u679c\u7a0b\u5e8f\u8fd0\u884c\u5730\u4e0d\u591f\u987a\u5229\uff0c\u8bf7\u603b\u7ed3\u95ee\u9898\u65e5\u5fd7\uff0c\u5e76\u7ed9\u6211\u4eec\u63d0\u4ea4 Issue \u606d\u559c\u4f60\uff0c\u4f60\u5df2\u7ecf\u7f16\u5199\u4e86\u4e00\u4e2a\u5c5e\u4e8e\u81ea\u5df1\u7684\u5f02\u6b65\u722c\u866b\uff0c\u662f\u4e0d\u662f\u5f88\u7b80\u5355\uff0c\u63a5\u4e0b\u6765\u4f60\u5c06\u5b9e\u9645\u7f16\u5199\u4e00\u4e2a\u4f8b\u5b50\uff0c\u4f1a\u66f4\u52a0\u6df1\u523b\u5730\u8ba4\u8bc6\u5230 Ruia \u7684\u5f3a\u5927\u4e4b\u5904\uff0c\u8bf7\u7ee7\u7eed\u9605\u8bfb Tutorials","title":"Getting help"},{"location":"cn/plugins.html","text":"Plugins \u6269\u5c55\u7684\u76ee\u7684\u662f\u5c06\u4e00\u4e9b\u5728\u722c\u866b\u7a0b\u5e8f\u4e2d\u9891\u7e41\u4f7f\u7528\u7684\u529f\u80fd\u5c01\u88c5\u8d77\u6765\u4f5c\u4e3a\u4e00\u4e2a\u6a21\u5757\u4f9b\u7b2c\u4e09\u65b9\u8c03\u7528\uff0c Ruia \u901a\u8fc7 Middleware \u6765\u8ba9\u5f00\u53d1\u8005\u5feb\u901f\u5730\u5b9e\u73b0\u7b2c\u4e09\u65b9\u6269\u5c55 \u524d\u9762\u4e00\u8282\u5df2\u7ecf\u8bf4\u8fc7\uff0c Middleware \u7684\u76ee\u7684\u662f\u5bf9\u6bcf\u6b21\u8bf7\u6c42\u524d\u540e\u8fdb\u884c\u4e00\u756a\u5904\u7406\uff0c\u7136\u540e\u6211\u4eec\u5b9e\u73b0\u4e86\u4e00\u4e2a\u529f\u80fd\uff0c\u5c31\u662f\u5728\u8bf7\u6c42\u5934\u91cc\u9762\u52a0\u5165 User-Agent \u53ef\u80fd\u4efb\u610f\u4e00\u4e2a\u722c\u866b\u90fd\u4f1a\u9700\u8981\u81ea\u52a8\u6dfb\u52a0\u968f\u673a User-Agent \u7684\u529f\u80fd\uff0c\u8ba9\u6211\u5c06\u8fd9\u4e2a\u529f\u80fd\u5c01\u88c5\u4e0b\uff0c\u4f7f\u5176\u6210\u4e3a Ruia \u7684\u4e00\u4e2a\u7b2c\u4e09\u65b9\u6269\u5c55\u5427\uff0c\u8ba9\u6211\u4eec\u73b0\u5728\u5c31\u5f00\u59cb\u5427 Creating a project \u9879\u76ee\u540d\u79f0\u4e3a\uff1a ruia-ua \uff0c\u56e0\u4e3a Ruia \u57fa\u4e8e Python3.6+ \uff0c\u6240\u4ee5\u6269\u5c55 ruia-ua \u4e5f\u4ea6\u7136\uff0c\u5047\u8bbe\u4f60\u6b64\u65f6\u4f7f\u7528\u7684\u662f Python3.6+ \uff0c\u8bf7\u6309\u7167\u5982\u4e0b\u64cd\u4f5c\uff1a # \u5b89\u88c5\u5305\u7ba1\u7406\u5de5\u5177 pipenv pip install pipenv # \u521b\u5efa\u9879\u76ee\u6587\u4ef6\u5939 mkdir ruia-ua cd ruia-ua # \u5b89\u88c5\u865a\u62df\u73af\u5883 pipenv install # \u5b89\u88c5 ruia pipenv install ruia # \u5b89\u88c5 aiofiles pipenv install aiofiles # \u521b\u5efa\u9879\u76ee\u76ee\u5f55 mkdir ruia_ua cd ruia_ua # \u5b9e\u73b0\u4ee3\u7801\u653e\u5728\u8fd9\u91cc touch __init__.py \u76ee\u5f55\u7ed3\u6784\u5982\u4e0b\uff1a ruia-ua \u251c\u2500\u2500 LICENSE # \u5f00\u6e90\u534f\u8bae \u251c\u2500\u2500 Pipfile # pipenv \u7ba1\u7406\u5de5\u5177\u751f\u6210\u6587\u4ef6 \u251c\u2500\u2500 Pipfile.lock \u251c\u2500\u2500 README.md \u251c\u2500\u2500 ruia_ua \u2502 \u251c\u2500\u2500 __init__.py # \u4ee3\u7801\u5b9e\u73b0 \u2502 \u2514\u2500\u2500 user_agents.txt # \u968f\u673aua\u96c6\u5408 \u2514\u2500\u2500 setup.py First extension user_agents.txt \u6587\u4ef6\u5305\u542b\u4e86\u5404\u79cd ua \uff0c\u63a5\u4e0b\u6765\u6211\u4eec\u53ea\u8981\u5229\u7528 ruia \u7684 Middleware \u5b9e\u73b0\u5728\u6bcf\u6b21\u8bf7\u6c42\u524d\u968f\u673a\u6dfb\u52a0\u4e00\u4e2a User-Agent \u5373\u53ef\uff0c\u5b9e\u73b0\u4ee3\u7801\u5982\u4e0b\uff1a import os import random import aiofiles from ruia import Middleware __version__ = \"0.0.1\" async def get_random_user_agent () -> str : \"\"\" Get a random user agent string. :return: Random user agent string. \"\"\" USER_AGENT = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.95 Safari/537.36' return random . choice ( await _get_data ( './user_agents.txt' , USER_AGENT )) async def _get_data ( filename : str , default : str ) -> list : \"\"\" Get data from all user_agents :param filename: filename :param default: default value :return: data \"\"\" root_folder = os . path . dirname ( __file__ ) user_agents_file = os . path . join ( root_folder , filename ) try : async with aiofiles . open ( user_agents_file , mode = 'r' ) as f : data = [ _ . strip () for _ in await f . readlines ()] except : data = [ default ] return data middleware = Middleware () @middleware.request async def add_random_ua ( request ): ua = await get_random_user_agent () if request . headers : request . headers . update ({ 'User-Agent' : ua }) else : request . headers = { 'User-Agent' : ua } \u7f16\u5199\u5b8c\u6210\u540e\uff0c\u6211\u4eec\u53ea\u9700\u8981\u5c06 ruia-ua \u4e0a\u4f20\u81f3\u793e\u533a\uff0c\u8fd9\u6837\u6240\u6709\u7684 ruia \u4f7f\u7528\u8005\u90fd\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528\u4f60\u7f16\u5199\u7684\u7b2c\u4e09\u65b9\u6269\u5c55\uff0c\u591a\u4e48\u7f8e\u597d\u7684\u4e00\u4ef6\u4e8b Usage \u6240\u6709\u7684\u722c\u866b\u7a0b\u5e8f\u90fd\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528 ruia-ua \u6765\u5b9e\u73b0\u81ea\u52a8\u6dfb\u52a0 User-Agent pip install ruia - ua \u4e3e\u4e2a\u5b9e\u9645\u4f7f\u7528\u7684\u4f8b\u5b50\uff1a from ruia import AttrField , TextField , Item , Spider from ruia_ua import middleware class HackerNewsItem ( Item ): target_item = TextField ( css_select = 'tr.athing' ) title = TextField ( css_select = 'a.storylink' ) url = AttrField ( css_select = 'a.storylink' , attr = 'href' ) async def clean_title ( self , value ): return value class HackerNewsSpider ( Spider ): start_urls = [ 'https://news.ycombinator.com/news?p=1' , 'https://news.ycombinator.com/news?p=2' ] concurrency = 10 async def parse ( self , response ): async for item in HackerNewsItem . get_items ( html = response . html ): print ( item . title ) if __name__ == '__main__' : HackerNewsSpider . start ( middleware = middleware ) \u7b2c\u4e09\u65b9\u6269\u5c55\u7684\u5b9e\u73b0\u5c06\u4f1a\u5927\u5927\u51cf\u5c11\u722c\u866b\u5de5\u7a0b\u5e08\u7684\u5f00\u53d1\u5468\u671f\uff0c ruia \u975e\u5e38\u5e0c\u671b\u4f60\u53ef\u4ee5\u5f00\u53d1\u5e76\u63d0\u4ea4\u81ea\u5df1\u7684\u7b2c\u4e09\u65b9\u6269\u5c55","title":"Plugins"},{"location":"cn/plugins.html#plugins","text":"\u6269\u5c55\u7684\u76ee\u7684\u662f\u5c06\u4e00\u4e9b\u5728\u722c\u866b\u7a0b\u5e8f\u4e2d\u9891\u7e41\u4f7f\u7528\u7684\u529f\u80fd\u5c01\u88c5\u8d77\u6765\u4f5c\u4e3a\u4e00\u4e2a\u6a21\u5757\u4f9b\u7b2c\u4e09\u65b9\u8c03\u7528\uff0c Ruia \u901a\u8fc7 Middleware \u6765\u8ba9\u5f00\u53d1\u8005\u5feb\u901f\u5730\u5b9e\u73b0\u7b2c\u4e09\u65b9\u6269\u5c55 \u524d\u9762\u4e00\u8282\u5df2\u7ecf\u8bf4\u8fc7\uff0c Middleware \u7684\u76ee\u7684\u662f\u5bf9\u6bcf\u6b21\u8bf7\u6c42\u524d\u540e\u8fdb\u884c\u4e00\u756a\u5904\u7406\uff0c\u7136\u540e\u6211\u4eec\u5b9e\u73b0\u4e86\u4e00\u4e2a\u529f\u80fd\uff0c\u5c31\u662f\u5728\u8bf7\u6c42\u5934\u91cc\u9762\u52a0\u5165 User-Agent \u53ef\u80fd\u4efb\u610f\u4e00\u4e2a\u722c\u866b\u90fd\u4f1a\u9700\u8981\u81ea\u52a8\u6dfb\u52a0\u968f\u673a User-Agent \u7684\u529f\u80fd\uff0c\u8ba9\u6211\u5c06\u8fd9\u4e2a\u529f\u80fd\u5c01\u88c5\u4e0b\uff0c\u4f7f\u5176\u6210\u4e3a Ruia \u7684\u4e00\u4e2a\u7b2c\u4e09\u65b9\u6269\u5c55\u5427\uff0c\u8ba9\u6211\u4eec\u73b0\u5728\u5c31\u5f00\u59cb\u5427","title":"Plugins"},{"location":"cn/plugins.html#creating-a-project","text":"\u9879\u76ee\u540d\u79f0\u4e3a\uff1a ruia-ua \uff0c\u56e0\u4e3a Ruia \u57fa\u4e8e Python3.6+ \uff0c\u6240\u4ee5\u6269\u5c55 ruia-ua \u4e5f\u4ea6\u7136\uff0c\u5047\u8bbe\u4f60\u6b64\u65f6\u4f7f\u7528\u7684\u662f Python3.6+ \uff0c\u8bf7\u6309\u7167\u5982\u4e0b\u64cd\u4f5c\uff1a # \u5b89\u88c5\u5305\u7ba1\u7406\u5de5\u5177 pipenv pip install pipenv # \u521b\u5efa\u9879\u76ee\u6587\u4ef6\u5939 mkdir ruia-ua cd ruia-ua # \u5b89\u88c5\u865a\u62df\u73af\u5883 pipenv install # \u5b89\u88c5 ruia pipenv install ruia # \u5b89\u88c5 aiofiles pipenv install aiofiles # \u521b\u5efa\u9879\u76ee\u76ee\u5f55 mkdir ruia_ua cd ruia_ua # \u5b9e\u73b0\u4ee3\u7801\u653e\u5728\u8fd9\u91cc touch __init__.py \u76ee\u5f55\u7ed3\u6784\u5982\u4e0b\uff1a ruia-ua \u251c\u2500\u2500 LICENSE # \u5f00\u6e90\u534f\u8bae \u251c\u2500\u2500 Pipfile # pipenv \u7ba1\u7406\u5de5\u5177\u751f\u6210\u6587\u4ef6 \u251c\u2500\u2500 Pipfile.lock \u251c\u2500\u2500 README.md \u251c\u2500\u2500 ruia_ua \u2502 \u251c\u2500\u2500 __init__.py # \u4ee3\u7801\u5b9e\u73b0 \u2502 \u2514\u2500\u2500 user_agents.txt # \u968f\u673aua\u96c6\u5408 \u2514\u2500\u2500 setup.py","title":"Creating a project"},{"location":"cn/plugins.html#first-extension","text":"user_agents.txt \u6587\u4ef6\u5305\u542b\u4e86\u5404\u79cd ua \uff0c\u63a5\u4e0b\u6765\u6211\u4eec\u53ea\u8981\u5229\u7528 ruia \u7684 Middleware \u5b9e\u73b0\u5728\u6bcf\u6b21\u8bf7\u6c42\u524d\u968f\u673a\u6dfb\u52a0\u4e00\u4e2a User-Agent \u5373\u53ef\uff0c\u5b9e\u73b0\u4ee3\u7801\u5982\u4e0b\uff1a import os import random import aiofiles from ruia import Middleware __version__ = \"0.0.1\" async def get_random_user_agent () -> str : \"\"\" Get a random user agent string. :return: Random user agent string. \"\"\" USER_AGENT = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.95 Safari/537.36' return random . choice ( await _get_data ( './user_agents.txt' , USER_AGENT )) async def _get_data ( filename : str , default : str ) -> list : \"\"\" Get data from all user_agents :param filename: filename :param default: default value :return: data \"\"\" root_folder = os . path . dirname ( __file__ ) user_agents_file = os . path . join ( root_folder , filename ) try : async with aiofiles . open ( user_agents_file , mode = 'r' ) as f : data = [ _ . strip () for _ in await f . readlines ()] except : data = [ default ] return data middleware = Middleware () @middleware.request async def add_random_ua ( request ): ua = await get_random_user_agent () if request . headers : request . headers . update ({ 'User-Agent' : ua }) else : request . headers = { 'User-Agent' : ua } \u7f16\u5199\u5b8c\u6210\u540e\uff0c\u6211\u4eec\u53ea\u9700\u8981\u5c06 ruia-ua \u4e0a\u4f20\u81f3\u793e\u533a\uff0c\u8fd9\u6837\u6240\u6709\u7684 ruia \u4f7f\u7528\u8005\u90fd\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528\u4f60\u7f16\u5199\u7684\u7b2c\u4e09\u65b9\u6269\u5c55\uff0c\u591a\u4e48\u7f8e\u597d\u7684\u4e00\u4ef6\u4e8b","title":"First extension"},{"location":"cn/plugins.html#usage","text":"\u6240\u6709\u7684\u722c\u866b\u7a0b\u5e8f\u90fd\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528 ruia-ua \u6765\u5b9e\u73b0\u81ea\u52a8\u6dfb\u52a0 User-Agent pip install ruia - ua \u4e3e\u4e2a\u5b9e\u9645\u4f7f\u7528\u7684\u4f8b\u5b50\uff1a from ruia import AttrField , TextField , Item , Spider from ruia_ua import middleware class HackerNewsItem ( Item ): target_item = TextField ( css_select = 'tr.athing' ) title = TextField ( css_select = 'a.storylink' ) url = AttrField ( css_select = 'a.storylink' , attr = 'href' ) async def clean_title ( self , value ): return value class HackerNewsSpider ( Spider ): start_urls = [ 'https://news.ycombinator.com/news?p=1' , 'https://news.ycombinator.com/news?p=2' ] concurrency = 10 async def parse ( self , response ): async for item in HackerNewsItem . get_items ( html = response . html ): print ( item . title ) if __name__ == '__main__' : HackerNewsSpider . start ( middleware = middleware ) \u7b2c\u4e09\u65b9\u6269\u5c55\u7684\u5b9e\u73b0\u5c06\u4f1a\u5927\u5927\u51cf\u5c11\u722c\u866b\u5de5\u7a0b\u5e08\u7684\u5f00\u53d1\u5468\u671f\uff0c ruia \u975e\u5e38\u5e0c\u671b\u4f60\u53ef\u4ee5\u5f00\u53d1\u5e76\u63d0\u4ea4\u81ea\u5df1\u7684\u7b2c\u4e09\u65b9\u6269\u5c55","title":"Usage"},{"location":"cn/tutorials.html","text":"Tutorials \u76ee\u6807\uff1a\u901a\u8fc7\u5bf9 Hacker News \u7684\u722c\u53d6\u6765\u5c55\u793a\u5982\u4f55\u4f7f\u7528 ruia \uff0c\u4e0b\u56fe\u7ea2\u6846\u4e2d\u7684\u6570\u636e\u5c31\u662f\u6211\u4eec\u9700\u8981\u722c\u53d6\u7684\uff1a \u5047\u8bbe\u6211\u4eec\u5c06\u6b64\u9879\u76ee\u547d\u540d\u4e3a hacker_news_spider \uff0c\u9879\u76ee\u7ed3\u6784\u5982\u4e0b\uff1a hacker_news_spider \u251c\u2500\u2500 db.py \u251c\u2500\u2500 hacker_news.py \u251c\u2500\u2500 items.py \u2514\u2500\u2500 middlewares.py Item Item \u7684\u76ee\u7684\u662f\u5b9a\u4e49\u76ee\u6807\u7f51\u7ad9\u4e2d\u4f60\u9700\u8981\u722c\u53d6\u7684\u6570\u636e\uff0c\u6b64\u65f6\uff0c\u722c\u866b\u7684\u76ee\u6807\u6570\u636e\u5c31\u662f\u9875\u9762\u4e2d\u7684 Title \u548c Url \uff0c\u600e\u4e48\u63d0\u53d6\u6570\u636e\uff0c ruia \u63d0\u4f9b\u4e86 CSS Selector \u548c XPath \u4e24\u79cd\u65b9\u5f0f\u63d0\u53d6\u76ee\u6807\u6570\u636e Notice: \u540e\u7eed\u722c\u866b\u4f8b\u5b50\u90fd\u9ed8\u8ba4\u4f7f\u7528CSS Selector\u7684\u89c4\u5219\u6765\u63d0\u53d6\u76ee\u6807\u6570\u636e \u8fd9\u91cc\u6211\u4eec\u4f7f\u7528 CSS Selector \u6765\u63d0\u53d6\u76ee\u6807\u6570\u636e\uff0c\u7528\u6d4f\u89c8\u5668\u6253\u5f00 Hacker News \uff0c\u53f3\u952e\u5ba1\u67e5\u5143\u7d20\uff1a \u663e\u800c\u6613\u89c1\uff0c\u6bcf\u9875\u5305\u542b 30 \u6761\u8d44\u8baf\uff0c\u90a3\u4e48\u76ee\u6807\u6570\u636e\u7684\u89c4\u5219\u53ef\u4ee5\u603b\u7ed3\u4e3a\uff1a Param Rule Description target_item tr.athing \u8868\u793a\u6bcf\u6761\u8d44\u8baf title a.storylink \u8868\u793a\u6bcf\u6761\u8d44\u8baf\u91cc\u7684\u6807\u9898 url a.storylink->href \u8868\u793a\u6bcf\u6761\u8d44\u8baf\u91cc\u6807\u9898\u7684\u94fe\u63a5 \u89c4\u5219\u660e\u786e\u4e4b\u540e\uff0c\u5c31\u53ef\u4ee5\u7528 Item \u6765\u5b9e\u73b0\u4e00\u4e2a\u9488\u5bf9\u4e8e\u76ee\u6807\u6570\u636e\u7684ORM\uff0c\u521b\u5efa\u6587\u4ef6 items.py \uff0c\u590d\u5236\u4e0b\u9762\u4ee3\u7801\uff1a from ruia import AttrField , TextField , Item class HackerNewsItem ( Item ): target_item = TextField ( css_select = 'tr.athing' ) title = TextField ( css_select = 'a.storylink' ) url = AttrField ( css_select = 'a.storylink' , attr = 'href' ) \u53ea\u9700\u8981\u7ee7\u627f Item \u7c7b\uff0c\u5c06\u76ee\u6807\u53c2\u6570\u5b9a\u4e49\u4e3a\u4e00\u4e2a\u5c5e\u6027\u5373\u53ef\uff0c\u5982\u679c\u76ee\u6807\u6570\u636e\u662f\u53ef\u4ee5\u5faa\u73af\u63d0\u53d6\u7684\uff0c\u6bd4\u5982\u6b64\u65f6\u6bcf\u4e00\u9875\u91cc\u9762\u6709 30 \u6761\u6570\u636e\uff0c\u90a3\u4e48\u5c31\u9700\u8981\u5b9a\u4e49 target_item \u6765\u5faa\u73af\u63d0\u53d6\u6bcf\u4e00\u6761\u6570\u636e\u91cc\u9762\u7684 Title \u548c Url Middleware Middleware \u7684\u76ee\u7684\u662f\u5bf9\u6bcf\u6b21\u8bf7\u6c42\u524d\u540e\u8fdb\u884c\u4e00\u756a\u5904\u7406\uff0c\u5206\u4e0b\u9762\u4e24\u79cd\u60c5\u51b5\uff1a \u5728\u6bcf\u6b21\u8bf7\u6c42\u4e4b\u524d\u505a\u4e00\u4e9b\u4e8b \u5728\u6bcf\u6b21\u8bf7\u6c42\u540e\u505a\u4e00\u4e9b\u4e8b \u6bd4\u5982\u6b64\u65f6\u722c\u53d6 Hacker News \uff0c\u4f60\u5e0c\u671b\u5728\u6bcf\u6b21\u8bf7\u6c42\u65f6\u5019\u81ea\u52a8\u6dfb\u52a0 Headers \u7684 User-Agent \uff0c\u53ef\u4ee5\u5c06\u4e0b\u9762\u4ee3\u7801\u590d\u5236\u5230\u4f60\u5efa\u7acb\u7684 middlewares.py \u6587\u4ef6\u4e2d\uff1a from ruia import Middleware middleware = Middleware () @middleware.request async def print_on_request ( request ): ua = 'ruia user-agent' request . headers . update ({ 'User-Agent' : ua }) \u8fd9\u6837\uff0c\u7a0b\u5e8f\u4f1a\u5728\u722c\u866b\u8bf7\u6c42\u7f51\u9875\u8d44\u6e90\u4e4b\u524d\u81ea\u52a8\u52a0\u4e0a User-Agent Database \u5bf9\u4e8e\u6570\u636e\u6301\u4e45\u5316\uff0c\u4f60\u53ef\u4ee5\u6309\u7167\u81ea\u5df1\u559c\u6b22\u7684\u65b9\u5f0f\u53bb\u505a\uff0c\u63a5\u4e0b\u6765\u6211\u4eec\u5c06\u4ee5 MongoDB \u4e3a\u4f8b\u5bf9\u722c\u53d6\u7684\u6570\u636e\u8fdb\u884c\u5b58\u50a8\uff0c\u521b\u5efa db.py \u6587\u4ef6\uff1a import asyncio from motor.motor_asyncio import AsyncIOMotorClient class MotorBase : \"\"\" About motor's doc: https://github.com/mongodb/motor \"\"\" _db = {} _collection = {} def __init__ ( self , loop = None ): self . motor_uri = '' self . loop = loop or asyncio . get_event_loop () def client ( self , db ): # motor self . motor_uri = f \"mongodb://localhost:27017/{db}\" return AsyncIOMotorClient ( self . motor_uri , io_loop = self . loop ) def get_db ( self , db = 'test' ): \"\"\" Get a db instance :param db: database name :return: the motor db instance \"\"\" if db not in self . _db : self . _db [ db ] = self . client ( db )[ db ] return self . _db [ db ] Spider Spider \u53ef\u4ee5\u8bf4\u662f\u722c\u866b\u7a0b\u5e8f\u7684\u5165\u53e3\uff0c\u5b83\u5c06 Item \u3001 Middleware \u3001 Request \u3001\u7b49\u6a21\u5757\u7ec4\u5408\u5728\u4e00\u8d77\uff0c\u4ece\u800c\u4e3a\u4f60\u6784\u9020\u4e00\u4e2a\u7a33\u5065\u7684\u722c\u866b\u7a0b\u5e8f \u8fd9\u6b21\u7684\u76ee\u7684\u4ec5\u4ec5\u662f\u4e3a\u4e86\u6f14\u793a\u5982\u4f55\u4f7f\u7528 ruia \u7f16\u5199\u722c\u866b\uff0c\u6240\u4ee5\u8fd9\u4e2a\u4f8b\u5b50\u4ec5\u4ec5\u722c\u53d6 Hacker News \u7684\u524d\u4e24\u9875\u6570\u636e\uff0c\u521b\u5efa hacker_news.py \u6587\u4ef6\uff1a from ruia import Request , Spider from items import HackerNewsItem from middlewares import middleware from db import MotorBase class HackerNewsSpider ( Spider ): start_urls = [ 'https://news.ycombinator.com' ] concurrency = 3 async def parse ( self , response ): self . mongo_db = MotorBase () . get_db ( 'ruia_test' ) urls = [ 'https://news.ycombinator.com/news?p=1' , 'https://news.ycombinator.com/news?p=2' ] for index , url in enumerate ( urls ): yield Request ( url , callback = self . parse_item , metadata = { 'index' : index } ) async def parse_item ( self , response ): async for item in HackerNewsItem . get_items ( html = response . html ): yield item async def process_item ( self , item ): try : await self . mongo_db . news . update_one ({ 'url' : item . url }, { '$set' : { 'url' : item . url , 'title' : item . title }}, upsert = True ) except Exception as e : self . logger . exception ( e ) if __name__ == '__main__' : HackerNewsSpider . start ( middleware = middleware ) HackerNewsSpider \u7ee7\u627f\u4e8e Spider \u7c7b\uff0c\u5176\u4e2d\u5b50\u7c7b\u5fc5\u987b\u5b9e\u73b0 parse() \u65b9\u6cd5\uff0c\u8fd0\u884c python hacker_news.py \uff1a [2018-09-24 17:59:19,865]-ruia-INFO spider : Spider started! [2018-09-24 17:59:19,866]-Request-INFO request: <GET: https://news.ycombinator.com> [2018-09-24 17:59:23,259]-Request-INFO request: <GET: https://news.ycombinator.com/news?p=1> [2018-09-24 17:59:23,260]-Request-INFO request: <GET: https://news.ycombinator.com/news?p=2> [2018-09-24 18:03:05,562]-ruia-INFO spider : Stopping spider: ruia [2018-09-24 18:03:05,562]-ruia-INFO spider : Total requests: 3 [2018-09-24 18:03:05,562]-ruia-INFO spider : Time usage: 0:00:02.802862 [2018-09-24 18:03:05,562]-ruia-INFO spider : Spider finished! \u6570\u636e\u5e93\u4e2d\u53ef\u4ee5\u770b\u5230\uff1a \u901a\u8fc7\u8fd9\u4e2a\u4f8b\u5b50\uff0c\u4f60\u5df2\u7ecf\u57fa\u672c\u638c\u63e1\u4e86 ruia \u7684 Item \u3001 Middleware \u3001 Request \u7b49\u6a21\u5757\u7684\u7528\u6cd5\uff0c\u7ed3\u5408\u81ea\u8eab\u9700\u6c42\uff0c\u4f60\u53ef\u4ee5\u7f16\u5199\u4efb\u4f55\u722c\u866b\uff0c\u4f8b\u5b50\u4ee3\u7801\u89c1 hacker_news_spider \u63a5\u4e0b\u6765\uff0c\u6211\u4eec\u5c06\u7ed3\u5408\u5b9e\u4f8b\uff0c\u7f16\u5199\u4e00\u4e2a ruia \u7684\u7b2c\u4e09\u65b9\u6269\u5c55\uff0c\u8be6\u89c1\uff1a Plugins","title":"Tutorials"},{"location":"cn/tutorials.html#tutorials","text":"\u76ee\u6807\uff1a\u901a\u8fc7\u5bf9 Hacker News \u7684\u722c\u53d6\u6765\u5c55\u793a\u5982\u4f55\u4f7f\u7528 ruia \uff0c\u4e0b\u56fe\u7ea2\u6846\u4e2d\u7684\u6570\u636e\u5c31\u662f\u6211\u4eec\u9700\u8981\u722c\u53d6\u7684\uff1a \u5047\u8bbe\u6211\u4eec\u5c06\u6b64\u9879\u76ee\u547d\u540d\u4e3a hacker_news_spider \uff0c\u9879\u76ee\u7ed3\u6784\u5982\u4e0b\uff1a hacker_news_spider \u251c\u2500\u2500 db.py \u251c\u2500\u2500 hacker_news.py \u251c\u2500\u2500 items.py \u2514\u2500\u2500 middlewares.py","title":"Tutorials"},{"location":"cn/tutorials.html#item","text":"Item \u7684\u76ee\u7684\u662f\u5b9a\u4e49\u76ee\u6807\u7f51\u7ad9\u4e2d\u4f60\u9700\u8981\u722c\u53d6\u7684\u6570\u636e\uff0c\u6b64\u65f6\uff0c\u722c\u866b\u7684\u76ee\u6807\u6570\u636e\u5c31\u662f\u9875\u9762\u4e2d\u7684 Title \u548c Url \uff0c\u600e\u4e48\u63d0\u53d6\u6570\u636e\uff0c ruia \u63d0\u4f9b\u4e86 CSS Selector \u548c XPath \u4e24\u79cd\u65b9\u5f0f\u63d0\u53d6\u76ee\u6807\u6570\u636e Notice: \u540e\u7eed\u722c\u866b\u4f8b\u5b50\u90fd\u9ed8\u8ba4\u4f7f\u7528CSS Selector\u7684\u89c4\u5219\u6765\u63d0\u53d6\u76ee\u6807\u6570\u636e \u8fd9\u91cc\u6211\u4eec\u4f7f\u7528 CSS Selector \u6765\u63d0\u53d6\u76ee\u6807\u6570\u636e\uff0c\u7528\u6d4f\u89c8\u5668\u6253\u5f00 Hacker News \uff0c\u53f3\u952e\u5ba1\u67e5\u5143\u7d20\uff1a \u663e\u800c\u6613\u89c1\uff0c\u6bcf\u9875\u5305\u542b 30 \u6761\u8d44\u8baf\uff0c\u90a3\u4e48\u76ee\u6807\u6570\u636e\u7684\u89c4\u5219\u53ef\u4ee5\u603b\u7ed3\u4e3a\uff1a Param Rule Description target_item tr.athing \u8868\u793a\u6bcf\u6761\u8d44\u8baf title a.storylink \u8868\u793a\u6bcf\u6761\u8d44\u8baf\u91cc\u7684\u6807\u9898 url a.storylink->href \u8868\u793a\u6bcf\u6761\u8d44\u8baf\u91cc\u6807\u9898\u7684\u94fe\u63a5 \u89c4\u5219\u660e\u786e\u4e4b\u540e\uff0c\u5c31\u53ef\u4ee5\u7528 Item \u6765\u5b9e\u73b0\u4e00\u4e2a\u9488\u5bf9\u4e8e\u76ee\u6807\u6570\u636e\u7684ORM\uff0c\u521b\u5efa\u6587\u4ef6 items.py \uff0c\u590d\u5236\u4e0b\u9762\u4ee3\u7801\uff1a from ruia import AttrField , TextField , Item class HackerNewsItem ( Item ): target_item = TextField ( css_select = 'tr.athing' ) title = TextField ( css_select = 'a.storylink' ) url = AttrField ( css_select = 'a.storylink' , attr = 'href' ) \u53ea\u9700\u8981\u7ee7\u627f Item \u7c7b\uff0c\u5c06\u76ee\u6807\u53c2\u6570\u5b9a\u4e49\u4e3a\u4e00\u4e2a\u5c5e\u6027\u5373\u53ef\uff0c\u5982\u679c\u76ee\u6807\u6570\u636e\u662f\u53ef\u4ee5\u5faa\u73af\u63d0\u53d6\u7684\uff0c\u6bd4\u5982\u6b64\u65f6\u6bcf\u4e00\u9875\u91cc\u9762\u6709 30 \u6761\u6570\u636e\uff0c\u90a3\u4e48\u5c31\u9700\u8981\u5b9a\u4e49 target_item \u6765\u5faa\u73af\u63d0\u53d6\u6bcf\u4e00\u6761\u6570\u636e\u91cc\u9762\u7684 Title \u548c Url","title":"Item"},{"location":"cn/tutorials.html#middleware","text":"Middleware \u7684\u76ee\u7684\u662f\u5bf9\u6bcf\u6b21\u8bf7\u6c42\u524d\u540e\u8fdb\u884c\u4e00\u756a\u5904\u7406\uff0c\u5206\u4e0b\u9762\u4e24\u79cd\u60c5\u51b5\uff1a \u5728\u6bcf\u6b21\u8bf7\u6c42\u4e4b\u524d\u505a\u4e00\u4e9b\u4e8b \u5728\u6bcf\u6b21\u8bf7\u6c42\u540e\u505a\u4e00\u4e9b\u4e8b \u6bd4\u5982\u6b64\u65f6\u722c\u53d6 Hacker News \uff0c\u4f60\u5e0c\u671b\u5728\u6bcf\u6b21\u8bf7\u6c42\u65f6\u5019\u81ea\u52a8\u6dfb\u52a0 Headers \u7684 User-Agent \uff0c\u53ef\u4ee5\u5c06\u4e0b\u9762\u4ee3\u7801\u590d\u5236\u5230\u4f60\u5efa\u7acb\u7684 middlewares.py \u6587\u4ef6\u4e2d\uff1a from ruia import Middleware middleware = Middleware () @middleware.request async def print_on_request ( request ): ua = 'ruia user-agent' request . headers . update ({ 'User-Agent' : ua }) \u8fd9\u6837\uff0c\u7a0b\u5e8f\u4f1a\u5728\u722c\u866b\u8bf7\u6c42\u7f51\u9875\u8d44\u6e90\u4e4b\u524d\u81ea\u52a8\u52a0\u4e0a User-Agent","title":"Middleware"},{"location":"cn/tutorials.html#database","text":"\u5bf9\u4e8e\u6570\u636e\u6301\u4e45\u5316\uff0c\u4f60\u53ef\u4ee5\u6309\u7167\u81ea\u5df1\u559c\u6b22\u7684\u65b9\u5f0f\u53bb\u505a\uff0c\u63a5\u4e0b\u6765\u6211\u4eec\u5c06\u4ee5 MongoDB \u4e3a\u4f8b\u5bf9\u722c\u53d6\u7684\u6570\u636e\u8fdb\u884c\u5b58\u50a8\uff0c\u521b\u5efa db.py \u6587\u4ef6\uff1a import asyncio from motor.motor_asyncio import AsyncIOMotorClient class MotorBase : \"\"\" About motor's doc: https://github.com/mongodb/motor \"\"\" _db = {} _collection = {} def __init__ ( self , loop = None ): self . motor_uri = '' self . loop = loop or asyncio . get_event_loop () def client ( self , db ): # motor self . motor_uri = f \"mongodb://localhost:27017/{db}\" return AsyncIOMotorClient ( self . motor_uri , io_loop = self . loop ) def get_db ( self , db = 'test' ): \"\"\" Get a db instance :param db: database name :return: the motor db instance \"\"\" if db not in self . _db : self . _db [ db ] = self . client ( db )[ db ] return self . _db [ db ]","title":"Database"},{"location":"cn/tutorials.html#spider","text":"Spider \u53ef\u4ee5\u8bf4\u662f\u722c\u866b\u7a0b\u5e8f\u7684\u5165\u53e3\uff0c\u5b83\u5c06 Item \u3001 Middleware \u3001 Request \u3001\u7b49\u6a21\u5757\u7ec4\u5408\u5728\u4e00\u8d77\uff0c\u4ece\u800c\u4e3a\u4f60\u6784\u9020\u4e00\u4e2a\u7a33\u5065\u7684\u722c\u866b\u7a0b\u5e8f \u8fd9\u6b21\u7684\u76ee\u7684\u4ec5\u4ec5\u662f\u4e3a\u4e86\u6f14\u793a\u5982\u4f55\u4f7f\u7528 ruia \u7f16\u5199\u722c\u866b\uff0c\u6240\u4ee5\u8fd9\u4e2a\u4f8b\u5b50\u4ec5\u4ec5\u722c\u53d6 Hacker News \u7684\u524d\u4e24\u9875\u6570\u636e\uff0c\u521b\u5efa hacker_news.py \u6587\u4ef6\uff1a from ruia import Request , Spider from items import HackerNewsItem from middlewares import middleware from db import MotorBase class HackerNewsSpider ( Spider ): start_urls = [ 'https://news.ycombinator.com' ] concurrency = 3 async def parse ( self , response ): self . mongo_db = MotorBase () . get_db ( 'ruia_test' ) urls = [ 'https://news.ycombinator.com/news?p=1' , 'https://news.ycombinator.com/news?p=2' ] for index , url in enumerate ( urls ): yield Request ( url , callback = self . parse_item , metadata = { 'index' : index } ) async def parse_item ( self , response ): async for item in HackerNewsItem . get_items ( html = response . html ): yield item async def process_item ( self , item ): try : await self . mongo_db . news . update_one ({ 'url' : item . url }, { '$set' : { 'url' : item . url , 'title' : item . title }}, upsert = True ) except Exception as e : self . logger . exception ( e ) if __name__ == '__main__' : HackerNewsSpider . start ( middleware = middleware ) HackerNewsSpider \u7ee7\u627f\u4e8e Spider \u7c7b\uff0c\u5176\u4e2d\u5b50\u7c7b\u5fc5\u987b\u5b9e\u73b0 parse() \u65b9\u6cd5\uff0c\u8fd0\u884c python hacker_news.py \uff1a [2018-09-24 17:59:19,865]-ruia-INFO spider : Spider started! [2018-09-24 17:59:19,866]-Request-INFO request: <GET: https://news.ycombinator.com> [2018-09-24 17:59:23,259]-Request-INFO request: <GET: https://news.ycombinator.com/news?p=1> [2018-09-24 17:59:23,260]-Request-INFO request: <GET: https://news.ycombinator.com/news?p=2> [2018-09-24 18:03:05,562]-ruia-INFO spider : Stopping spider: ruia [2018-09-24 18:03:05,562]-ruia-INFO spider : Total requests: 3 [2018-09-24 18:03:05,562]-ruia-INFO spider : Time usage: 0:00:02.802862 [2018-09-24 18:03:05,562]-ruia-INFO spider : Spider finished! \u6570\u636e\u5e93\u4e2d\u53ef\u4ee5\u770b\u5230\uff1a \u901a\u8fc7\u8fd9\u4e2a\u4f8b\u5b50\uff0c\u4f60\u5df2\u7ecf\u57fa\u672c\u638c\u63e1\u4e86 ruia \u7684 Item \u3001 Middleware \u3001 Request \u7b49\u6a21\u5757\u7684\u7528\u6cd5\uff0c\u7ed3\u5408\u81ea\u8eab\u9700\u6c42\uff0c\u4f60\u53ef\u4ee5\u7f16\u5199\u4efb\u4f55\u722c\u866b\uff0c\u4f8b\u5b50\u4ee3\u7801\u89c1 hacker_news_spider \u63a5\u4e0b\u6765\uff0c\u6211\u4eec\u5c06\u7ed3\u5408\u5b9e\u4f8b\uff0c\u7f16\u5199\u4e00\u4e2a ruia \u7684\u7b2c\u4e09\u65b9\u6269\u5c55\uff0c\u8be6\u89c1\uff1a Plugins","title":"Spider"},{"location":"cn/topics/item.html","text":"Item Item \u7684\u4e3b\u8981\u4f5c\u7528\u662f\u5b9a\u4e49\u4ee5\u53ca\u901a\u8fc7\u4e00\u5b9a\u7684\u89c4\u5219\u63d0\u53d6\u6e90\u7f51\u9875\u4e2d\u7684\u76ee\u6807\u6570\u636e\uff0c\u5b83\u4e3b\u8981\u63d0\u4f9b\u4e00\u4e0b\u4e24\u4e2a\u65b9\u6cd5\uff1a - get_item \uff1a\u9488\u5bf9\u9875\u9762\u5355\u76ee\u6807\u6570\u636e\u8fdb\u884c\u63d0\u53d6 - get_items \uff1a\u9488\u5bf9\u9875\u9762\u591a\u76ee\u6807\u6570\u636e\u8fdb\u884c\u63d0\u53d6 Core arguments get_item \u548c get_items \u65b9\u6cd5\u63a5\u6536\u7684\u53c2\u6570\u662f\u4e00\u81f4\u7684\uff1a - html\uff1a\u7f51\u9875\u6e90\u7801 - url\uff1a\u7f51\u9875\u94fe\u63a5 - html_etree\uff1aetree._Element\u5bf9\u8c61 Usage \u901a\u8fc7\u4e0a\u9762\u7684\u53c2\u6570\u4ecb\u7ecd\u53ef\u4ee5\u77e5\u9053\uff0c\u4e0d\u8bba\u662f\u6e90\u7f51\u7ad9\u94fe\u63a5\u6216\u8005\u7f51\u7ad9 HTML \u6e90\u7801\uff0c\u751a\u81f3\u662f\u7ecf\u8fc7 lxml \u5904\u7406\u8fc7\u7684 etree._Element \u5bf9\u8c61\uff0c Item \u80fd\u63a5\u6536\u8fd9\u4e09\u79cd\u7c7b\u578b\u7684\u8f93\u5165\u5e76\u8fdb\u884c\u5904\u7406 import asyncio from ruia import AttrField , TextField , Item class HackerNewsItem ( Item ): target_item = TextField ( css_select = 'tr.athing' ) title = TextField ( css_select = 'a.storylink' ) url = AttrField ( css_select = 'a.storylink' , attr = 'href' ) async def clean_title ( self , value ): return value async_func = HackerNewsItem . get_items ( url = \"https://news.ycombinator.com/\" ) items = asyncio . get_event_loop () . run_until_complete ( async_func ) for item in items : print ( item . title , item . url ) \u6709\u65f6\u4f60\u4f1a\u9047\u89c1\u8fd9\u6837\u4e00\u79cd\u60c5\u51b5\uff0c\u4f8b\u5982\u722c\u53d6Github\u7684Issue\u65f6\uff0c\u4f60\u4f1a\u53d1\u73b0\u4e00\u4e2aIssue\u53ef\u80fd\u5bf9\u5e94\u591a\u4e2aTag\u3002 \u8fd9\u65f6\uff0c\u5c06Tag\u4f5c\u4e3a\u4e00\u4e2a\u72ec\u7acb\u7684 Item \u6765\u63d0\u53d6\u662f\u4e0d\u5212\u7b97\u7684\uff0c \u6211\u4eec\u53ef\u4ee5\u4f7f\u7528 Field \u5b57\u6bb5\u7684 many=True \u53c2\u6570\uff0c\u4f7f\u8fd9\u4e2a\u5b57\u6bb5\u8fd4\u56de\u4e00\u4e2a\u5217\u8868\u3002 import asyncio from ruia import Item , TextField , AttrField class GithiubIssueItem ( Item ): title = TextField ( css_select = 'title' ) tags = AttrField ( css_select = 'a.IssueLabel' , attr = 'data-name' , many = True ) item = asyncio . run ( GithiubIssueItem . get_item ( url = 'https://github.com/pypa/pip/issues/72' )) assert isinstance ( item . tags , list ) \u540c\u6837\uff0c TextField \u4e5f\u652f\u6301 many \u53c2\u6570\u3002 How It Works? \u6700\u7ec8 Item \u7c7b\u4f1a\u5c06\u8f93\u5165\u6700\u7ec8\u8f6c\u5316\u4e3a etree._Element \u5bf9\u8c61\u8fdb\u884c\u5904\u7406\uff0c\u7136\u540e\u5229\u7528\u5143\u7c7b\u7684\u601d\u60f3\u5c06\u6bcf\u4e00\u4e2a Field \u6784\u9020\u7684\u5c5e\u6027\u8ba1\u7b97\u4e3a\u6e90\u7f51\u9875\u4e0a\u5bf9\u5e94\u7684\u771f\u5b9e\u6570\u636e","title":"Item"},{"location":"cn/topics/item.html#item","text":"Item \u7684\u4e3b\u8981\u4f5c\u7528\u662f\u5b9a\u4e49\u4ee5\u53ca\u901a\u8fc7\u4e00\u5b9a\u7684\u89c4\u5219\u63d0\u53d6\u6e90\u7f51\u9875\u4e2d\u7684\u76ee\u6807\u6570\u636e\uff0c\u5b83\u4e3b\u8981\u63d0\u4f9b\u4e00\u4e0b\u4e24\u4e2a\u65b9\u6cd5\uff1a - get_item \uff1a\u9488\u5bf9\u9875\u9762\u5355\u76ee\u6807\u6570\u636e\u8fdb\u884c\u63d0\u53d6 - get_items \uff1a\u9488\u5bf9\u9875\u9762\u591a\u76ee\u6807\u6570\u636e\u8fdb\u884c\u63d0\u53d6","title":"Item"},{"location":"cn/topics/item.html#core-arguments","text":"get_item \u548c get_items \u65b9\u6cd5\u63a5\u6536\u7684\u53c2\u6570\u662f\u4e00\u81f4\u7684\uff1a - html\uff1a\u7f51\u9875\u6e90\u7801 - url\uff1a\u7f51\u9875\u94fe\u63a5 - html_etree\uff1aetree._Element\u5bf9\u8c61","title":"Core arguments"},{"location":"cn/topics/item.html#usage","text":"\u901a\u8fc7\u4e0a\u9762\u7684\u53c2\u6570\u4ecb\u7ecd\u53ef\u4ee5\u77e5\u9053\uff0c\u4e0d\u8bba\u662f\u6e90\u7f51\u7ad9\u94fe\u63a5\u6216\u8005\u7f51\u7ad9 HTML \u6e90\u7801\uff0c\u751a\u81f3\u662f\u7ecf\u8fc7 lxml \u5904\u7406\u8fc7\u7684 etree._Element \u5bf9\u8c61\uff0c Item \u80fd\u63a5\u6536\u8fd9\u4e09\u79cd\u7c7b\u578b\u7684\u8f93\u5165\u5e76\u8fdb\u884c\u5904\u7406 import asyncio from ruia import AttrField , TextField , Item class HackerNewsItem ( Item ): target_item = TextField ( css_select = 'tr.athing' ) title = TextField ( css_select = 'a.storylink' ) url = AttrField ( css_select = 'a.storylink' , attr = 'href' ) async def clean_title ( self , value ): return value async_func = HackerNewsItem . get_items ( url = \"https://news.ycombinator.com/\" ) items = asyncio . get_event_loop () . run_until_complete ( async_func ) for item in items : print ( item . title , item . url ) \u6709\u65f6\u4f60\u4f1a\u9047\u89c1\u8fd9\u6837\u4e00\u79cd\u60c5\u51b5\uff0c\u4f8b\u5982\u722c\u53d6Github\u7684Issue\u65f6\uff0c\u4f60\u4f1a\u53d1\u73b0\u4e00\u4e2aIssue\u53ef\u80fd\u5bf9\u5e94\u591a\u4e2aTag\u3002 \u8fd9\u65f6\uff0c\u5c06Tag\u4f5c\u4e3a\u4e00\u4e2a\u72ec\u7acb\u7684 Item \u6765\u63d0\u53d6\u662f\u4e0d\u5212\u7b97\u7684\uff0c \u6211\u4eec\u53ef\u4ee5\u4f7f\u7528 Field \u5b57\u6bb5\u7684 many=True \u53c2\u6570\uff0c\u4f7f\u8fd9\u4e2a\u5b57\u6bb5\u8fd4\u56de\u4e00\u4e2a\u5217\u8868\u3002 import asyncio from ruia import Item , TextField , AttrField class GithiubIssueItem ( Item ): title = TextField ( css_select = 'title' ) tags = AttrField ( css_select = 'a.IssueLabel' , attr = 'data-name' , many = True ) item = asyncio . run ( GithiubIssueItem . get_item ( url = 'https://github.com/pypa/pip/issues/72' )) assert isinstance ( item . tags , list ) \u540c\u6837\uff0c TextField \u4e5f\u652f\u6301 many \u53c2\u6570\u3002","title":"Usage"},{"location":"cn/topics/item.html#how-it-works","text":"\u6700\u7ec8 Item \u7c7b\u4f1a\u5c06\u8f93\u5165\u6700\u7ec8\u8f6c\u5316\u4e3a etree._Element \u5bf9\u8c61\u8fdb\u884c\u5904\u7406\uff0c\u7136\u540e\u5229\u7528\u5143\u7c7b\u7684\u601d\u60f3\u5c06\u6bcf\u4e00\u4e2a Field \u6784\u9020\u7684\u5c5e\u6027\u8ba1\u7b97\u4e3a\u6e90\u7f51\u9875\u4e0a\u5bf9\u5e94\u7684\u771f\u5b9e\u6570\u636e","title":"How It Works?"},{"location":"cn/topics/middleware.html","text":"Middleware Middleware \u7684\u4e3b\u8981\u4f5c\u7528\u662f\u5728\u8fdb\u884c\u4e00\u4e2a\u8bf7\u6c42\u7684\u524d\u540e\u8fdb\u884c\u4e00\u4e9b\u5904\u7406\uff0c\u6bd4\u5982\u76d1\u542c\u8bf7\u6c42\u6216\u8005\u54cd\u5e94\uff1a - Middleware().request \uff1a\u5728\u8bf7\u6c42\u524d\u5904\u7406\u4e00\u4e9b\u4e8b\u60c5 - Middleware().response \uff1a\u5728\u8bf7\u6c42\u540e\u5904\u7406\u4e00\u4e9b\u4e8b\u60c5 Usage \u4f7f\u7528\u4e2d\u95f4\u4ef6\u6709\u4e24\u70b9\u9700\u8981\u6ce8\u610f\uff0c\u4e00\u4e2a\u662f\u5904\u7406\u51fd\u6570\u9700\u8981\u5e26\u4e0a\u7279\u5b9a\u7684\u53c2\u6570\uff0c\u7b2c\u4e8c\u4e2a\u662f\u4e0d\u9700\u8981\u8fd4\u56de\u503c\uff0c\u5177\u4f53\u4f7f\u7528\u5982\u4e0b\uff1a from ruia import Middleware middleware = Middleware () @middleware.request async def print_on_request ( request ): \"\"\" \u6bcf\u6b21\u8bf7\u6c42\u524d\u90fd\u4f1a\u8c03\u7528\u6b64\u51fd\u6570 request: Request\u7c7b\u7684\u5b9e\u4f8b\u5bf9\u8c61 \"\"\" print ( \"request: print when a request is received\" ) @middleware.response async def print_on_response ( request , response ): \"\"\" \u6bcf\u6b21\u8bf7\u6c42\u540e\u90fd\u4f1a\u8c03\u7528\u6b64\u51fd\u6570 request: Request\u7c7b\u7684\u5b9e\u4f8b\u5bf9\u8c61 response: Response\u7c7b\u7684\u5b9e\u4f8b\u5bf9\u8c61 \"\"\" print ( \"response: print when a response is received\" ) How It Works? Middleware \u901a\u8fc7\u88c5\u9970\u5668\u6765\u5b9e\u73b0\u5bf9\u51fd\u6570\u7684\u56de\u8c03\uff0c\u4ece\u800c\u8ba9\u5f00\u53d1\u8005\u53ef\u4ee5\u4f18\u96c5\u7684\u5b9e\u73b0\u4e2d\u95f4\u4ef6\u529f\u80fd\uff0c Middleware \u7c7b\u4e2d\u7684\u4e24\u4e2a\u5c5e\u6027 request_middleware \u548c response_middleware \u5206\u522b\u7ef4\u62a4\u7740\u4e00\u4e2a\u961f\u5217\u6765\u5904\u7406\u5f00\u53d1\u8005\u5b9a\u4e49\u7684\u5904\u7406\u51fd\u6570","title":"Middleware"},{"location":"cn/topics/middleware.html#middleware","text":"Middleware \u7684\u4e3b\u8981\u4f5c\u7528\u662f\u5728\u8fdb\u884c\u4e00\u4e2a\u8bf7\u6c42\u7684\u524d\u540e\u8fdb\u884c\u4e00\u4e9b\u5904\u7406\uff0c\u6bd4\u5982\u76d1\u542c\u8bf7\u6c42\u6216\u8005\u54cd\u5e94\uff1a - Middleware().request \uff1a\u5728\u8bf7\u6c42\u524d\u5904\u7406\u4e00\u4e9b\u4e8b\u60c5 - Middleware().response \uff1a\u5728\u8bf7\u6c42\u540e\u5904\u7406\u4e00\u4e9b\u4e8b\u60c5","title":"Middleware"},{"location":"cn/topics/middleware.html#usage","text":"\u4f7f\u7528\u4e2d\u95f4\u4ef6\u6709\u4e24\u70b9\u9700\u8981\u6ce8\u610f\uff0c\u4e00\u4e2a\u662f\u5904\u7406\u51fd\u6570\u9700\u8981\u5e26\u4e0a\u7279\u5b9a\u7684\u53c2\u6570\uff0c\u7b2c\u4e8c\u4e2a\u662f\u4e0d\u9700\u8981\u8fd4\u56de\u503c\uff0c\u5177\u4f53\u4f7f\u7528\u5982\u4e0b\uff1a from ruia import Middleware middleware = Middleware () @middleware.request async def print_on_request ( request ): \"\"\" \u6bcf\u6b21\u8bf7\u6c42\u524d\u90fd\u4f1a\u8c03\u7528\u6b64\u51fd\u6570 request: Request\u7c7b\u7684\u5b9e\u4f8b\u5bf9\u8c61 \"\"\" print ( \"request: print when a request is received\" ) @middleware.response async def print_on_response ( request , response ): \"\"\" \u6bcf\u6b21\u8bf7\u6c42\u540e\u90fd\u4f1a\u8c03\u7528\u6b64\u51fd\u6570 request: Request\u7c7b\u7684\u5b9e\u4f8b\u5bf9\u8c61 response: Response\u7c7b\u7684\u5b9e\u4f8b\u5bf9\u8c61 \"\"\" print ( \"response: print when a response is received\" )","title":"Usage"},{"location":"cn/topics/middleware.html#how-it-works","text":"Middleware \u901a\u8fc7\u88c5\u9970\u5668\u6765\u5b9e\u73b0\u5bf9\u51fd\u6570\u7684\u56de\u8c03\uff0c\u4ece\u800c\u8ba9\u5f00\u53d1\u8005\u53ef\u4ee5\u4f18\u96c5\u7684\u5b9e\u73b0\u4e2d\u95f4\u4ef6\u529f\u80fd\uff0c Middleware \u7c7b\u4e2d\u7684\u4e24\u4e2a\u5c5e\u6027 request_middleware \u548c response_middleware \u5206\u522b\u7ef4\u62a4\u7740\u4e00\u4e2a\u961f\u5217\u6765\u5904\u7406\u5f00\u53d1\u8005\u5b9a\u4e49\u7684\u5904\u7406\u51fd\u6570","title":"How It Works?"},{"location":"cn/topics/request.html","text":"Request Request \u7684\u4e3b\u8981\u4f5c\u7528\u662f\u65b9\u4fbf\u5730\u5904\u7406\u7f51\u7edc\u8bf7\u6c42\uff0c\u6700\u7ec8\u8fd4\u56de\u4e00\u4e2a Response \u5bf9\u8c61\u3002 \u4e3b\u8981\u63d0\u4f9b\u7684\u65b9\u6cd5\u6709\uff1a - Request().fetch \uff1a\u8bf7\u6c42\u4e00\u4e2a\u7f51\u9875\u8d44\u6e90\uff0c\u53ef\u4ee5\u5355\u72ec\u4f7f\u7528 - Request().fetch_callback \uff1a\u4e3a Spider \u7c7b\u63d0\u4f9b\u7684\u548c\u6838\u5fc3\u65b9\u6cd5 Core arguments url\uff1a\u8bf7\u6c42\u7684\u8d44\u6e90\u94fe\u63a5 method\uff1a\u8bf7\u6c42\u7684\u65b9\u6cd5\uff0c GET \u6216\u8005 POST callback\uff1a\u56de\u8c03\u51fd\u6570 headers\uff1a\u8bf7\u6c42\u5934 load_js\uff1a\u76ee\u6807\u7f51\u9875\u662f\u5426\u9700\u8981\u52a0\u8f7djs metadata\uff1a\u8de8\u8bf7\u6c42\u4f20\u9012\u7684\u4e00\u4e9b\u6570\u636e request_config\uff1a\u8bf7\u6c42\u914d\u7f6e request_session\uff1a aiohttp \u7684\u8bf7\u6c42session kwargs\uff1a\u8bf7\u6c42\u76ee\u6807\u8d44\u6e90\u53ef\u5b9a\u4e49\u7684\u5176\u4ed6\u53c2\u6570 Usage \u901a\u8fc7\u4e0a\u9762\u7684\u53c2\u6570\u4ecb\u7ecd\u53ef\u4ee5\u77e5\u9053\uff0c Request \u9664\u4e86\u9700\u8981\u7ed3\u5408 Spider \u4f7f\u7528\uff0c\u4e5f\u53ef\u4ee5\u5355\u72ec\u4f7f\u7528\uff1a import asyncio from ruia import Request request = Request ( \"https://news.ycombinator.com/\" ) response = asyncio . get_event_loop () . run_until_complete ( request . fetch ()) # Output # [2018-07-25 11:23:42,620]-Request-INFO <GET: https://news.ycombinator.com/> # <Response url[text]: https://news.ycombinator.com/ status:200 metadata:{}> How It Works? Request \u901a\u8fc7\u5bf9 aiohttp \u548c pyppeteer \u7684\u5c01\u88c5\u6765\u5b9e\u73b0\u5bf9\u7f51\u9875\u8d44\u6e90\u7684\u5f02\u6b65\u8bf7\u6c42","title":"Request"},{"location":"cn/topics/request.html#request","text":"Request \u7684\u4e3b\u8981\u4f5c\u7528\u662f\u65b9\u4fbf\u5730\u5904\u7406\u7f51\u7edc\u8bf7\u6c42\uff0c\u6700\u7ec8\u8fd4\u56de\u4e00\u4e2a Response \u5bf9\u8c61\u3002 \u4e3b\u8981\u63d0\u4f9b\u7684\u65b9\u6cd5\u6709\uff1a - Request().fetch \uff1a\u8bf7\u6c42\u4e00\u4e2a\u7f51\u9875\u8d44\u6e90\uff0c\u53ef\u4ee5\u5355\u72ec\u4f7f\u7528 - Request().fetch_callback \uff1a\u4e3a Spider \u7c7b\u63d0\u4f9b\u7684\u548c\u6838\u5fc3\u65b9\u6cd5","title":"Request"},{"location":"cn/topics/request.html#core-arguments","text":"url\uff1a\u8bf7\u6c42\u7684\u8d44\u6e90\u94fe\u63a5 method\uff1a\u8bf7\u6c42\u7684\u65b9\u6cd5\uff0c GET \u6216\u8005 POST callback\uff1a\u56de\u8c03\u51fd\u6570 headers\uff1a\u8bf7\u6c42\u5934 load_js\uff1a\u76ee\u6807\u7f51\u9875\u662f\u5426\u9700\u8981\u52a0\u8f7djs metadata\uff1a\u8de8\u8bf7\u6c42\u4f20\u9012\u7684\u4e00\u4e9b\u6570\u636e request_config\uff1a\u8bf7\u6c42\u914d\u7f6e request_session\uff1a aiohttp \u7684\u8bf7\u6c42session kwargs\uff1a\u8bf7\u6c42\u76ee\u6807\u8d44\u6e90\u53ef\u5b9a\u4e49\u7684\u5176\u4ed6\u53c2\u6570","title":"Core arguments"},{"location":"cn/topics/request.html#usage","text":"\u901a\u8fc7\u4e0a\u9762\u7684\u53c2\u6570\u4ecb\u7ecd\u53ef\u4ee5\u77e5\u9053\uff0c Request \u9664\u4e86\u9700\u8981\u7ed3\u5408 Spider \u4f7f\u7528\uff0c\u4e5f\u53ef\u4ee5\u5355\u72ec\u4f7f\u7528\uff1a import asyncio from ruia import Request request = Request ( \"https://news.ycombinator.com/\" ) response = asyncio . get_event_loop () . run_until_complete ( request . fetch ()) # Output # [2018-07-25 11:23:42,620]-Request-INFO <GET: https://news.ycombinator.com/> # <Response url[text]: https://news.ycombinator.com/ status:200 metadata:{}>","title":"Usage"},{"location":"cn/topics/request.html#how-it-works","text":"Request \u901a\u8fc7\u5bf9 aiohttp \u548c pyppeteer \u7684\u5c01\u88c5\u6765\u5b9e\u73b0\u5bf9\u7f51\u9875\u8d44\u6e90\u7684\u5f02\u6b65\u8bf7\u6c42","title":"How It Works?"},{"location":"cn/topics/response.html","text":"Response Response \u7684\u76ee\u7684\u662f\u8fd4\u56de\u4e00\u4e2a\u7edf\u4e00\u4e14\u53cb\u597d\u7684\u54cd\u5e94\u5bf9\u8c61\uff0c\u4e3b\u8981\u5c5e\u6027\u5982\u4e0b\uff1a - url\uff1a\u8bf7\u6c42\u7684\u8d44\u6e90\u94fe\u63a5 - metadata\uff1a\u8de8\u8bf7\u6c42\u4f20\u9012\u7684\u4e00\u4e9b\u6570\u636e - html\uff1a\u6e90\u7f51\u7ad9\u8fd4\u56de\u7684\u8d44\u6e90\u6570\u636e - cookies\uff1a\u7f51\u7ad9 cookies - history\uff1a\u8bbf\u95ee\u5386\u53f2 - headers\uff1a\u8bf7\u6c42\u5934 - status\uff1a\u8bf7\u6c42\u72b6\u6001\u7801","title":"Response"},{"location":"cn/topics/response.html#response","text":"Response \u7684\u76ee\u7684\u662f\u8fd4\u56de\u4e00\u4e2a\u7edf\u4e00\u4e14\u53cb\u597d\u7684\u54cd\u5e94\u5bf9\u8c61\uff0c\u4e3b\u8981\u5c5e\u6027\u5982\u4e0b\uff1a - url\uff1a\u8bf7\u6c42\u7684\u8d44\u6e90\u94fe\u63a5 - metadata\uff1a\u8de8\u8bf7\u6c42\u4f20\u9012\u7684\u4e00\u4e9b\u6570\u636e - html\uff1a\u6e90\u7f51\u7ad9\u8fd4\u56de\u7684\u8d44\u6e90\u6570\u636e - cookies\uff1a\u7f51\u7ad9 cookies - history\uff1a\u8bbf\u95ee\u5386\u53f2 - headers\uff1a\u8bf7\u6c42\u5934 - status\uff1a\u8bf7\u6c42\u72b6\u6001\u7801","title":"Response"},{"location":"cn/topics/selector.html","text":"Selector Selector \u901a\u8fc7 Field \u7c7b\u5b9e\u73b0\uff0c\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86 CSS Selector \u548c XPath \u4e24\u79cd\u65b9\u5f0f\u63d0\u53d6\u76ee\u6807\u6570\u636e\uff0c\u5177\u4f53\u7531\u4e0b\u9762\u4e24\u4e2a\u7c7b\u5b9e\u73b0\uff1a - AttrField(BaseField) \uff1a\u63d0\u53d6\u7f51\u9875\u6807\u7b7e\u7684\u5c5e\u6027\u6570\u636e - TextField(BaseField) \uff1a\u63d0\u53d6\u7f51\u9875\u6807\u7b7e\u7684text\u6570\u636e Core arguments \u6240\u6709\u7684 Field \u5171\u6709\u7684\u53c2\u6570\uff1a - default: str, \u8bbe\u7f6e\u9ed8\u8ba4\u503c\uff0c\u5efa\u8bae\u5b9a\u4e49\uff0c\u5426\u5219\u627e\u4e0d\u5230\u5b57\u6bb5\u65f6\u4f1a\u62a5\u9519 - many: bool, \u8fd4\u56de\u503c\u5c06\u662f\u4e00\u4e2a\u5217\u8868 AttrField \u3001 TextField \u3001 HtmlField \u5171\u7528\u53c2\u6570\uff1a - css_select\uff1astr, \u5229\u7528 CSS Selector \u63d0\u53d6\u76ee\u6807\u6570\u636e - xpath_select\uff1astr, \u5229\u7528 XPath \u63d0\u53d6\u76ee\u6807\u6570\u636e AttrField \u9700\u8981\u4e00\u4e2a\u989d\u5916\u7684\u53c2\u6570\uff1a - attr\uff1a\u76ee\u6807\u6807\u7b7e\u5c5e\u6027 RegexField \u9700\u8981\u4e00\u4e2a\u989d\u5916\u7684\u53c2\u6570\uff1a - re_select: str, \u6b63\u5219\u8868\u8fbe\u5f0f\u5b57\u7b26\u4e32 Usage from lxml import etree from ruia import AttrField , TextField , HtmlField , RegexField HTML = \"\"\" <html> <head> <title>ruia</title> </head> <body>\u00ac <p> <a class=\"test_link\" href=\"https://github.com/howie6879/ruia\">hello github.</a> </p> </body> </html> \"\"\" html = etree . HTML ( HTML ) def test_css_select (): field = TextField ( css_select = \"head title\" ) value = field . extract ( html_etree = html ) assert value == \"ruia\" def test_xpath_select (): field = TextField ( xpath_select = '/html/head/title' ) value = field . extract ( html_etree = html ) assert value == \"ruia\" def test_attr_field (): attr_field = AttrField ( css_select = \"p a.test_link\" , attr = 'href' ) value = attr_field . extract ( html_etree = html ) assert value == \"https://github.com/howie6879/ruia\" def test_html_field (): field = HtmlField ( css_select = \"a.test_link\" ) assert field . extract ( html_etree = html ) == '<a class=\"test_link\" href=\"https://github.com/howie6879/ruia\">hello github.</a>' def test_re_field (): field = RegexField ( re_select = '<title>(.*?)</title>' ) href = field . extract ( html = HTML ) assert href == 'ruia' How It Works? \u5b9a\u597d CSS Selector \u6216 XPath \u89c4\u5219\uff0c\u7136\u540e\u5229\u7528 lxml \u5b9e\u73b0\u5bf9\u76ee\u6807 html \u8fdb\u884c\u76ee\u6807\u6570\u636e\u7684\u63d0\u53d6 \u5173\u4e8e RegexField \u8be6\u7ec6\u4fe1\u606f\u8bf7\u53c2\u9605 \u82f1\u6587\u6587\u6863 \u3002","title":"Selector"},{"location":"cn/topics/selector.html#selector","text":"Selector \u901a\u8fc7 Field \u7c7b\u5b9e\u73b0\uff0c\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86 CSS Selector \u548c XPath \u4e24\u79cd\u65b9\u5f0f\u63d0\u53d6\u76ee\u6807\u6570\u636e\uff0c\u5177\u4f53\u7531\u4e0b\u9762\u4e24\u4e2a\u7c7b\u5b9e\u73b0\uff1a - AttrField(BaseField) \uff1a\u63d0\u53d6\u7f51\u9875\u6807\u7b7e\u7684\u5c5e\u6027\u6570\u636e - TextField(BaseField) \uff1a\u63d0\u53d6\u7f51\u9875\u6807\u7b7e\u7684text\u6570\u636e","title":"Selector"},{"location":"cn/topics/selector.html#core-arguments","text":"\u6240\u6709\u7684 Field \u5171\u6709\u7684\u53c2\u6570\uff1a - default: str, \u8bbe\u7f6e\u9ed8\u8ba4\u503c\uff0c\u5efa\u8bae\u5b9a\u4e49\uff0c\u5426\u5219\u627e\u4e0d\u5230\u5b57\u6bb5\u65f6\u4f1a\u62a5\u9519 - many: bool, \u8fd4\u56de\u503c\u5c06\u662f\u4e00\u4e2a\u5217\u8868 AttrField \u3001 TextField \u3001 HtmlField \u5171\u7528\u53c2\u6570\uff1a - css_select\uff1astr, \u5229\u7528 CSS Selector \u63d0\u53d6\u76ee\u6807\u6570\u636e - xpath_select\uff1astr, \u5229\u7528 XPath \u63d0\u53d6\u76ee\u6807\u6570\u636e AttrField \u9700\u8981\u4e00\u4e2a\u989d\u5916\u7684\u53c2\u6570\uff1a - attr\uff1a\u76ee\u6807\u6807\u7b7e\u5c5e\u6027 RegexField \u9700\u8981\u4e00\u4e2a\u989d\u5916\u7684\u53c2\u6570\uff1a - re_select: str, \u6b63\u5219\u8868\u8fbe\u5f0f\u5b57\u7b26\u4e32","title":"Core arguments"},{"location":"cn/topics/selector.html#usage","text":"from lxml import etree from ruia import AttrField , TextField , HtmlField , RegexField HTML = \"\"\" <html> <head> <title>ruia</title> </head> <body>\u00ac <p> <a class=\"test_link\" href=\"https://github.com/howie6879/ruia\">hello github.</a> </p> </body> </html> \"\"\" html = etree . HTML ( HTML ) def test_css_select (): field = TextField ( css_select = \"head title\" ) value = field . extract ( html_etree = html ) assert value == \"ruia\" def test_xpath_select (): field = TextField ( xpath_select = '/html/head/title' ) value = field . extract ( html_etree = html ) assert value == \"ruia\" def test_attr_field (): attr_field = AttrField ( css_select = \"p a.test_link\" , attr = 'href' ) value = attr_field . extract ( html_etree = html ) assert value == \"https://github.com/howie6879/ruia\" def test_html_field (): field = HtmlField ( css_select = \"a.test_link\" ) assert field . extract ( html_etree = html ) == '<a class=\"test_link\" href=\"https://github.com/howie6879/ruia\">hello github.</a>' def test_re_field (): field = RegexField ( re_select = '<title>(.*?)</title>' ) href = field . extract ( html = HTML ) assert href == 'ruia'","title":"Usage"},{"location":"cn/topics/selector.html#how-it-works","text":"\u5b9a\u597d CSS Selector \u6216 XPath \u89c4\u5219\uff0c\u7136\u540e\u5229\u7528 lxml \u5b9e\u73b0\u5bf9\u76ee\u6807 html \u8fdb\u884c\u76ee\u6807\u6570\u636e\u7684\u63d0\u53d6","title":"How It Works?"},{"location":"cn/topics/selector.html#regexfield","text":"\u8be6\u7ec6\u4fe1\u606f\u8bf7\u53c2\u9605 \u82f1\u6587\u6587\u6863 \u3002","title":"\u5173\u4e8eRegexField"},{"location":"cn/topics/spider.html","text":"Spider Spider \u662f\u722c\u866b\u7a0b\u5e8f\u7684\u5165\u53e3\uff0c\u5b83\u5c06Item\u3001Middleware\u3001Request\u3001\u7b49\u6a21\u5757\u7ec4\u5408\u5728\u4e00\u8d77\uff0c\u4ece\u800c\u4e3a\u4f60\u6784\u9020\u4e00\u4e2a\u7a33\u5065\u7684\u722c\u866b\u7a0b\u5e8f\u3002\u4f60\u53ea\u9700\u8981\u5173\u6ce8\u4ee5\u4e0b\u4e24\u4e2a\u51fd\u6570\uff1a - Spider.start \uff1a\u722c\u866b\u7684\u542f\u52a8\u51fd\u6570 - parse \uff1a\u722c\u866b\u7684\u7b2c\u4e00\u5c42\u89e3\u6790\u51fd\u6570\uff0c\u7ee7\u627f Spider \u7684\u5b50\u7c7b\u5fc5\u987b\u5b9e\u73b0\u8fd9\u4e2a\u51fd\u6570 Core arguments Spider.start \u7684\u53c2\u6570\u5982\u4e0b\uff1a - after_start\uff1a\u722c\u866b\u542f\u52a8\u540e\u7684\u94a9\u5b50\u51fd\u6570 - before_stop\uff1a\u722c\u866b\u542f\u52a8\u524d\u7684\u94a9\u5b50\u51fd\u6570 - middleware\uff1a\u4e2d\u95f4\u4ef6\u7c7b\uff0c\u53ef\u4ee5\u662f\u4e00\u4e2a\u4e2d\u95f4\u4ef6 Middleware() \u5b9e\u4f8b\uff0c\u4e5f\u53ef\u4ee5\u662f\u4e00\u7ec4 Middleware() \u5b9e\u4f8b\u7ec4\u6210\u7684\u5217\u8868 - loop\uff1a\u4e8b\u4ef6\u5faa\u73af Usage import aiofiles from ruia import AttrField , TextField , Item , Spider class HackerNewsItem ( Item ): target_item = TextField ( css_select = 'tr.athing' ) title = TextField ( css_select = 'a.storylink' ) url = AttrField ( css_select = 'a.storylink' , attr = 'href' ) async def clean_title ( self , value ): return value class HackerNewsSpider ( Spider ): start_urls = [ 'https://news.ycombinator.com/news?p=1' , 'https://news.ycombinator.com/news?p=2' ] async def parse ( self , response ): async for item in HackerNewsItem . get_items ( html = response . html ): yield item async def process_item ( self , item : HackerNewsItem ): \"\"\"Ruia build-in method\"\"\" async with aiofiles . open ( './hacker_news.txt' , 'a' ) as f : await f . write ( str ( item . title ) + ' \\n ' ) if __name__ == '__main__' : HackerNewsSpider . start () How It Works? Spider \u4f1a\u81ea\u52a8\u8bfb\u53d6 start_urls \u5217\u8868\u91cc\u9762\u7684\u8bf7\u6c42\u94fe\u63a5\uff0c\u7136\u540e\u7ef4\u62a4\u4e00\u4e2a\u5f02\u6b65\u961f\u5217\uff0c\u4f7f\u7528\u751f\u4ea7\u6d88\u8d39\u8005\u6a21\u5f0f\u8fdb\u884c\u722c\u53d6\uff0c\u722c\u866b\u7a0b\u5e8f\u4e00\u76f4\u5faa\u73af\u76f4\u5230\u6ca1\u6709\u8c03\u7528\u51fd\u6570\u4e3a\u6b62","title":"Spider"},{"location":"cn/topics/spider.html#spider","text":"Spider \u662f\u722c\u866b\u7a0b\u5e8f\u7684\u5165\u53e3\uff0c\u5b83\u5c06Item\u3001Middleware\u3001Request\u3001\u7b49\u6a21\u5757\u7ec4\u5408\u5728\u4e00\u8d77\uff0c\u4ece\u800c\u4e3a\u4f60\u6784\u9020\u4e00\u4e2a\u7a33\u5065\u7684\u722c\u866b\u7a0b\u5e8f\u3002\u4f60\u53ea\u9700\u8981\u5173\u6ce8\u4ee5\u4e0b\u4e24\u4e2a\u51fd\u6570\uff1a - Spider.start \uff1a\u722c\u866b\u7684\u542f\u52a8\u51fd\u6570 - parse \uff1a\u722c\u866b\u7684\u7b2c\u4e00\u5c42\u89e3\u6790\u51fd\u6570\uff0c\u7ee7\u627f Spider \u7684\u5b50\u7c7b\u5fc5\u987b\u5b9e\u73b0\u8fd9\u4e2a\u51fd\u6570","title":"Spider"},{"location":"cn/topics/spider.html#core-arguments","text":"Spider.start \u7684\u53c2\u6570\u5982\u4e0b\uff1a - after_start\uff1a\u722c\u866b\u542f\u52a8\u540e\u7684\u94a9\u5b50\u51fd\u6570 - before_stop\uff1a\u722c\u866b\u542f\u52a8\u524d\u7684\u94a9\u5b50\u51fd\u6570 - middleware\uff1a\u4e2d\u95f4\u4ef6\u7c7b\uff0c\u53ef\u4ee5\u662f\u4e00\u4e2a\u4e2d\u95f4\u4ef6 Middleware() \u5b9e\u4f8b\uff0c\u4e5f\u53ef\u4ee5\u662f\u4e00\u7ec4 Middleware() \u5b9e\u4f8b\u7ec4\u6210\u7684\u5217\u8868 - loop\uff1a\u4e8b\u4ef6\u5faa\u73af","title":"Core arguments"},{"location":"cn/topics/spider.html#usage","text":"import aiofiles from ruia import AttrField , TextField , Item , Spider class HackerNewsItem ( Item ): target_item = TextField ( css_select = 'tr.athing' ) title = TextField ( css_select = 'a.storylink' ) url = AttrField ( css_select = 'a.storylink' , attr = 'href' ) async def clean_title ( self , value ): return value class HackerNewsSpider ( Spider ): start_urls = [ 'https://news.ycombinator.com/news?p=1' , 'https://news.ycombinator.com/news?p=2' ] async def parse ( self , response ): async for item in HackerNewsItem . get_items ( html = response . html ): yield item async def process_item ( self , item : HackerNewsItem ): \"\"\"Ruia build-in method\"\"\" async with aiofiles . open ( './hacker_news.txt' , 'a' ) as f : await f . write ( str ( item . title ) + ' \\n ' ) if __name__ == '__main__' : HackerNewsSpider . start ()","title":"Usage"},{"location":"cn/topics/spider.html#how-it-works","text":"Spider \u4f1a\u81ea\u52a8\u8bfb\u53d6 start_urls \u5217\u8868\u91cc\u9762\u7684\u8bf7\u6c42\u94fe\u63a5\uff0c\u7136\u540e\u7ef4\u62a4\u4e00\u4e2a\u5f02\u6b65\u961f\u5217\uff0c\u4f7f\u7528\u751f\u4ea7\u6d88\u8d39\u8005\u6a21\u5f0f\u8fdb\u884c\u722c\u53d6\uff0c\u722c\u866b\u7a0b\u5e8f\u4e00\u76f4\u5faa\u73af\u76f4\u5230\u6ca1\u6709\u8c03\u7528\u51fd\u6570\u4e3a\u6b62","title":"How It Works?"},{"location":"en/quickstart.html","text":"Create a Typical Ruia Spider Let's fetch some news from Hacker News in four steps: Define item Test item Write spider Run Step 1: Define Item After analyzing HTML structure, we define the following data item. The skill of analyzing HTML structure is important for a spider engineer, Ruia believe you have already had this skill, and won't talk about it here. from ruia import Item , TextField , AttrField class HackerNewsItem ( Item ): target_item = TextField ( css_select = 'tr.athing' ) title = TextField ( css_select = 'a.storylink' ) url = AttrField ( css_select = 'a.storylink' , attr = 'href' ) It's easy to understand: we want to get an item from HTML structure, the item contains two fields: title and url . Wait! What is target_item ? target_item is a built-in Ruia field , indicates that the HTML element matched by its selectors contains one item. In this example, we are crawling a catalogue of Hacker News, and there are many news items in one page. target_item tells Ruia to focus on these HTML elements when extracting field. Step 2: Test Item Ruia is a low-coupling web crawling framework. Each class can be used separately in your project. You can even write a simple spider with only ruia.Item , ruia.TextField and ruia.AttrField . This feature provides a convenient way to test HackerNewsItem . import asyncio from ruia import Item , TextField , AttrField class HackerNewsItem ( Item ): target_item = TextField ( css_select = 'tr.athing' ) title = TextField ( css_select = 'a.storylink' ) url = AttrField ( css_select = 'a.storylink' , attr = 'href' ) async def test_item (): url = 'https://news.ycombinator.com/news?p=1' async for item in HackerNewsItem . get_items ( url = url ): print ( '{}: {}' . format ( item . title , item . url )) if __name__ == '__main__' : # Python 3.7 Required. asyncio . run ( test_item ()) # For Python 3.6 # loop = asyncio.get_event_loop() # loop.run_until_complete(test_item()) Waiting for the output in your console. Step 3: Write Spider Ruia.spider is used to control requests and responses, such as concurrency control. It's important for a spider, or you will be banned by the server in one minute. By default, the concurrency is 3. \"\"\" Target: https://news.ycombinator.com/ pip install aiofiles \"\"\" import aiofiles from ruia import Item , TextField , AttrField , Spider class HackerNewsItem ( Item ): target_item = TextField ( css_select = 'tr.athing' ) title = TextField ( css_select = 'a.storylink' ) url = AttrField ( css_select = 'a.storylink' , attr = 'href' ) class HackerNewsSpider ( Spider ): concurrency = 2 start_urls = [ f 'https://news.ycombinator.com/news?p={index}' for index in range ( 3 )] async def parse ( self , response ): async for item in HackerNewsItem . get_items ( html = response . html ): yield item async def process_item ( self , item : HackerNewsItem ): \"\"\"Ruia build-in method\"\"\" async with aiofiles . open ( './hacker_news.txt' , 'a' ) as f : await f . write ( str ( item . title ) + ' \\n ' ) Just define a property of the subclass of Spider . In this example, we crawl in two coroutines. If you do not know coroutine, as a crawler engineer, ruia believe you know the threading pool of spider. Coroutine is a more efficient way to behave like threading pool. parse(self, response) is the entry point of a spider. After starting a spider, it send requests to web server. Once received a response, ruia.Spider will call its parse function to extract data from HTML source code. Step 4: Run Now everything is ready. Run! import aiofiles from ruia import Item , TextField , AttrField , Spider class HackerNewsItem ( Item ): target_item = TextField ( css_select = 'tr.athing' ) title = TextField ( css_select = 'a.storylink' ) url = AttrField ( css_select = 'a.storylink' , attr = 'href' ) class HackerNewsSpider ( Spider ): concurrency = 2 start_urls = [ f 'https://news.ycombinator.com/news?p={index}' for index in range ( 3 )] async def parse ( self , response ): async for item in HackerNewsItem . get_items ( html = response . html ): yield item async def process_item ( self , item : HackerNewsItem ): \"\"\"Ruia build-in method\"\"\" async with aiofiles . open ( './hacker_news.txt' , 'a' ) as f : await f . write ( str ( item . title ) + ' \\n ' ) if __name__ == '__main__' : HackerNewsSpider . start () Hey, notice that, do not run Spider.start() in a await statement! It's just a normal function ! You just create a spider in one python file! Amazing!","title":"Quick Start"},{"location":"en/quickstart.html#create-a-typical-ruia-spider","text":"Let's fetch some news from Hacker News in four steps: Define item Test item Write spider Run","title":"Create a Typical Ruia Spider"},{"location":"en/quickstart.html#step-1-define-item","text":"After analyzing HTML structure, we define the following data item. The skill of analyzing HTML structure is important for a spider engineer, Ruia believe you have already had this skill, and won't talk about it here. from ruia import Item , TextField , AttrField class HackerNewsItem ( Item ): target_item = TextField ( css_select = 'tr.athing' ) title = TextField ( css_select = 'a.storylink' ) url = AttrField ( css_select = 'a.storylink' , attr = 'href' ) It's easy to understand: we want to get an item from HTML structure, the item contains two fields: title and url . Wait! What is target_item ? target_item is a built-in Ruia field , indicates that the HTML element matched by its selectors contains one item. In this example, we are crawling a catalogue of Hacker News, and there are many news items in one page. target_item tells Ruia to focus on these HTML elements when extracting field.","title":"Step 1: Define Item"},{"location":"en/quickstart.html#step-2-test-item","text":"Ruia is a low-coupling web crawling framework. Each class can be used separately in your project. You can even write a simple spider with only ruia.Item , ruia.TextField and ruia.AttrField . This feature provides a convenient way to test HackerNewsItem . import asyncio from ruia import Item , TextField , AttrField class HackerNewsItem ( Item ): target_item = TextField ( css_select = 'tr.athing' ) title = TextField ( css_select = 'a.storylink' ) url = AttrField ( css_select = 'a.storylink' , attr = 'href' ) async def test_item (): url = 'https://news.ycombinator.com/news?p=1' async for item in HackerNewsItem . get_items ( url = url ): print ( '{}: {}' . format ( item . title , item . url )) if __name__ == '__main__' : # Python 3.7 Required. asyncio . run ( test_item ()) # For Python 3.6 # loop = asyncio.get_event_loop() # loop.run_until_complete(test_item()) Waiting for the output in your console.","title":"Step 2: Test Item"},{"location":"en/quickstart.html#step-3-write-spider","text":"Ruia.spider is used to control requests and responses, such as concurrency control. It's important for a spider, or you will be banned by the server in one minute. By default, the concurrency is 3. \"\"\" Target: https://news.ycombinator.com/ pip install aiofiles \"\"\" import aiofiles from ruia import Item , TextField , AttrField , Spider class HackerNewsItem ( Item ): target_item = TextField ( css_select = 'tr.athing' ) title = TextField ( css_select = 'a.storylink' ) url = AttrField ( css_select = 'a.storylink' , attr = 'href' ) class HackerNewsSpider ( Spider ): concurrency = 2 start_urls = [ f 'https://news.ycombinator.com/news?p={index}' for index in range ( 3 )] async def parse ( self , response ): async for item in HackerNewsItem . get_items ( html = response . html ): yield item async def process_item ( self , item : HackerNewsItem ): \"\"\"Ruia build-in method\"\"\" async with aiofiles . open ( './hacker_news.txt' , 'a' ) as f : await f . write ( str ( item . title ) + ' \\n ' ) Just define a property of the subclass of Spider . In this example, we crawl in two coroutines. If you do not know coroutine, as a crawler engineer, ruia believe you know the threading pool of spider. Coroutine is a more efficient way to behave like threading pool. parse(self, response) is the entry point of a spider. After starting a spider, it send requests to web server. Once received a response, ruia.Spider will call its parse function to extract data from HTML source code.","title":"Step 3: Write Spider"},{"location":"en/quickstart.html#step-4-run","text":"Now everything is ready. Run! import aiofiles from ruia import Item , TextField , AttrField , Spider class HackerNewsItem ( Item ): target_item = TextField ( css_select = 'tr.athing' ) title = TextField ( css_select = 'a.storylink' ) url = AttrField ( css_select = 'a.storylink' , attr = 'href' ) class HackerNewsSpider ( Spider ): concurrency = 2 start_urls = [ f 'https://news.ycombinator.com/news?p={index}' for index in range ( 3 )] async def parse ( self , response ): async for item in HackerNewsItem . get_items ( html = response . html ): yield item async def process_item ( self , item : HackerNewsItem ): \"\"\"Ruia build-in method\"\"\" async with aiofiles . open ( './hacker_news.txt' , 'a' ) as f : await f . write ( str ( item . title ) + ' \\n ' ) if __name__ == '__main__' : HackerNewsSpider . start () Hey, notice that, do not run Spider.start() in a await statement! It's just a normal function ! You just create a spider in one python file! Amazing!","title":"Step 4: Run"},{"location":"en/apis/field.html","text":"Define Data with Fields Overview Fields are used to extract value from HTML code. Ruia supports the following fields: TextField : extract text string of the selected HTML element AttrField : extract an attribute of the selected HTML element HtmlField : extract raw HTML code of the selected HTML element RegexField : use standard library re for better performance Note All the parameters of fields are keyword arguments . TextField TextField first select an HTML element by CSS Selector or XPath Selector, then get the text value of the selected element. Parameters css_select : str , alternative, match HTML element(s) with CSS Selector xpath_select : str , alternative, match HTML element(s) with XPath Selector default : str , recommended, the default value if nothing matched in HTML element many : bool , optional, extract a list if True Example import ruia from lxml import etree HTML = ''' <body> <div class=\"title\">Ruia Documentation</div> <ul> <li class=\"tag\" href=\"./easy.html\">easy</li> <li class=\"tag\" href=\"./fast.html\">fast</li> <li class=\"tag\" href=\"./powerful.html\">powerful</li> </ul> </body> ''' html = etree . HTML ( HTML ) def test_text_field (): title_field = ruia . TextField ( css_select = '.title' , default = 'Untitled' ) assert title_field . extract ( html_etree = html ) == 'Ruia Documentation' tag_field = ruia . TextField ( css_select = '.tag' , default = 'No tag' , many = True ) assert tag_field . extract ( html_etree = html ) == [ 'easy' , 'fast' , 'powerful' ] AttrField TextField first select an HTML element by CSS Selector or XPath Selector, then get the attribute value of the selected element. Parameters attr : str , required, the name of the attribute you want to extract css_select : str , alternative, match HTML element(s) with CSS Selector xpath_select : str , alternative, match HTML element(s) with XPath Selector default : str , recommended, the default value if nothing matched in HTML element many : bool , optional, extract a list if True Example import ruia from lxml import etree HTML = ''' <body> <div class=\"title\" href=\"/\">Ruia Documentation</div> <ul> <li class=\"tag\" href=\"./easy.html\">easy</li> <li class=\"tag\" href=\"./fast.html\">fast</li> <li class=\"tag\" href=\"./powerful.html\">powerful</li> </ul> </body> ''' html = etree . HTML ( HTML ) def test_attr_field (): title = ruia . AttrField ( css_select = '.title' , attr = 'href' , default = 'Untitled' ) assert title . extract ( html_etree = html ) == '/' tags = ruia . AttrField ( css_select = '.tag' , attr = 'href' , default = 'No tag' , many = True ) assert tags . extract ( html_etree = html )[ 0 ] == './easy.html' HtmlField TextField first select an HTML element by CSS Selector or XPath Selector, then get the raw HTML code of the selected element. If there's some spaces or some text outside any HTML elements between this element and next element, then this part of text will also inside the return value. It's an unstable feature, perhaps in later versions the outside text will be remove by default. Parameters css_select : str , alternative, match HTML element(s) with CSS Selector xpath_select : str , alternative, match HTML element(s) with XPath Selector default : str , recommended, the default value if nothing matched in HTML element many : bool , optional, extract a list if True Example import ruia from lxml import etree HTML = ''' <body> <div class=\"title\">Ruia Documentation</div> <ul> <li class=\"tag\" href=\"./easy.html\">easy</li> <li class=\"tag\" href=\"./fast.html\">fast</li> <li class=\"tag\" href=\"./powerful.html\">powerful</li> </ul> </body> ''' html = etree . HTML ( HTML ) def test_html_field (): title = ruia . HtmlField ( css_select = '.title' , default = 'Untitled' ) assert title . extract ( html_etree = html ) == '<div class=\"title\" href=\"/\">Ruia Documentation</div> \\n ' tags = ruia . HtmlField ( css_select = '.tag' , default = 'No tag' , many = True ) assert tags . extract ( html_etree = html )[ 1 ] == '<li class=\"tag\" href=\"./fast.html\">fast</li> \\n ' RegexField TextField do not parse html structure, it directly use python standard library re . If your spider meets performance limitation, try RegexField . However, ruia is based on asyncio , you will seldom meet performance limitation! RegexField has a complex behaviour: if no group: return the whole matched string if regex has a group: return the group value if regex has multiple groups: return a list a string if regex has named groups, no matter one or more: return a dict, whose key and value are both string if many=True , return a list of above values Parameters re_select : str , required, match HTML element(s) with regular expression default : str , recommended, the default value if nothing matched in HTML element many : bool , optional, extract a list if True Example import ruia from lxml import etree HTML = ''' <body> <div class=\"title\" href=\"/\">Ruia Documentation</div> <ul> <li class=\"tag\" href=\"./easy.html\">easy</li> <li class=\"tag\" href=\"./fast.html\">fast</li> <li class=\"tag\" href=\"./powerful.html\">powerful</li> </ul> </body> ''' html = etree . HTML ( HTML ) def test_regex_field (): title = ruia . RegexField ( re_select = '<div class=\"title\" href=\"(.*?)\">(.*?)</div>' ) assert title . extract ( html = HTML )[ 0 ] == '/' assert title . extract ( html = HTML )[ 1 ] == 'Ruia Documentation' tags = ruia . RegexField ( re_select = '<li class=\"tag\" href=\"(?P<href>.*?)\">(?P<text>.*?)</li>' , many = True ) result = tags . extract ( html = HTML ) assert isinstance ( result , list ) assert len ( result ) == 3 assert isinstance ( result [ 0 ], dict ) assert result [ 0 ][ 'href' ] == './easy.html' About Parameter many Parameter many=False indicates if the field will extract one value or multiple values from HTML source code. For example, one Github Issue has many tags, We can use Item.get_items to get multiple values of tags, but that means an extra class definition. Parameter many aims to solve this problem. A field is default by many=False , that means, for TextField , AttrField and HtmlField , Field.extract(*, **) will always return a string, and RegexField will return a string or a list or dict, depending on whether there are groups in the regular expression. We can consider it with a 'singular number'. With many=True , each field will return a 'plural', that is, return a list.","title":"Field"},{"location":"en/apis/field.html#define-data-with-fields","text":"","title":"Define Data with Fields"},{"location":"en/apis/field.html#overview","text":"Fields are used to extract value from HTML code. Ruia supports the following fields: TextField : extract text string of the selected HTML element AttrField : extract an attribute of the selected HTML element HtmlField : extract raw HTML code of the selected HTML element RegexField : use standard library re for better performance Note All the parameters of fields are keyword arguments .","title":"Overview"},{"location":"en/apis/field.html#textfield","text":"TextField first select an HTML element by CSS Selector or XPath Selector, then get the text value of the selected element.","title":"TextField"},{"location":"en/apis/field.html#parameters","text":"css_select : str , alternative, match HTML element(s) with CSS Selector xpath_select : str , alternative, match HTML element(s) with XPath Selector default : str , recommended, the default value if nothing matched in HTML element many : bool , optional, extract a list if True","title":"Parameters"},{"location":"en/apis/field.html#example","text":"import ruia from lxml import etree HTML = ''' <body> <div class=\"title\">Ruia Documentation</div> <ul> <li class=\"tag\" href=\"./easy.html\">easy</li> <li class=\"tag\" href=\"./fast.html\">fast</li> <li class=\"tag\" href=\"./powerful.html\">powerful</li> </ul> </body> ''' html = etree . HTML ( HTML ) def test_text_field (): title_field = ruia . TextField ( css_select = '.title' , default = 'Untitled' ) assert title_field . extract ( html_etree = html ) == 'Ruia Documentation' tag_field = ruia . TextField ( css_select = '.tag' , default = 'No tag' , many = True ) assert tag_field . extract ( html_etree = html ) == [ 'easy' , 'fast' , 'powerful' ]","title":"Example"},{"location":"en/apis/field.html#attrfield","text":"TextField first select an HTML element by CSS Selector or XPath Selector, then get the attribute value of the selected element.","title":"AttrField"},{"location":"en/apis/field.html#parameters_1","text":"attr : str , required, the name of the attribute you want to extract css_select : str , alternative, match HTML element(s) with CSS Selector xpath_select : str , alternative, match HTML element(s) with XPath Selector default : str , recommended, the default value if nothing matched in HTML element many : bool , optional, extract a list if True","title":"Parameters"},{"location":"en/apis/field.html#example_1","text":"import ruia from lxml import etree HTML = ''' <body> <div class=\"title\" href=\"/\">Ruia Documentation</div> <ul> <li class=\"tag\" href=\"./easy.html\">easy</li> <li class=\"tag\" href=\"./fast.html\">fast</li> <li class=\"tag\" href=\"./powerful.html\">powerful</li> </ul> </body> ''' html = etree . HTML ( HTML ) def test_attr_field (): title = ruia . AttrField ( css_select = '.title' , attr = 'href' , default = 'Untitled' ) assert title . extract ( html_etree = html ) == '/' tags = ruia . AttrField ( css_select = '.tag' , attr = 'href' , default = 'No tag' , many = True ) assert tags . extract ( html_etree = html )[ 0 ] == './easy.html'","title":"Example"},{"location":"en/apis/field.html#htmlfield","text":"TextField first select an HTML element by CSS Selector or XPath Selector, then get the raw HTML code of the selected element. If there's some spaces or some text outside any HTML elements between this element and next element, then this part of text will also inside the return value. It's an unstable feature, perhaps in later versions the outside text will be remove by default.","title":"HtmlField"},{"location":"en/apis/field.html#parameters_2","text":"css_select : str , alternative, match HTML element(s) with CSS Selector xpath_select : str , alternative, match HTML element(s) with XPath Selector default : str , recommended, the default value if nothing matched in HTML element many : bool , optional, extract a list if True","title":"Parameters"},{"location":"en/apis/field.html#example_2","text":"import ruia from lxml import etree HTML = ''' <body> <div class=\"title\">Ruia Documentation</div> <ul> <li class=\"tag\" href=\"./easy.html\">easy</li> <li class=\"tag\" href=\"./fast.html\">fast</li> <li class=\"tag\" href=\"./powerful.html\">powerful</li> </ul> </body> ''' html = etree . HTML ( HTML ) def test_html_field (): title = ruia . HtmlField ( css_select = '.title' , default = 'Untitled' ) assert title . extract ( html_etree = html ) == '<div class=\"title\" href=\"/\">Ruia Documentation</div> \\n ' tags = ruia . HtmlField ( css_select = '.tag' , default = 'No tag' , many = True ) assert tags . extract ( html_etree = html )[ 1 ] == '<li class=\"tag\" href=\"./fast.html\">fast</li> \\n '","title":"Example"},{"location":"en/apis/field.html#regexfield","text":"TextField do not parse html structure, it directly use python standard library re . If your spider meets performance limitation, try RegexField . However, ruia is based on asyncio , you will seldom meet performance limitation! RegexField has a complex behaviour: if no group: return the whole matched string if regex has a group: return the group value if regex has multiple groups: return a list a string if regex has named groups, no matter one or more: return a dict, whose key and value are both string if many=True , return a list of above values","title":"RegexField"},{"location":"en/apis/field.html#parameters_3","text":"re_select : str , required, match HTML element(s) with regular expression default : str , recommended, the default value if nothing matched in HTML element many : bool , optional, extract a list if True","title":"Parameters"},{"location":"en/apis/field.html#example_3","text":"import ruia from lxml import etree HTML = ''' <body> <div class=\"title\" href=\"/\">Ruia Documentation</div> <ul> <li class=\"tag\" href=\"./easy.html\">easy</li> <li class=\"tag\" href=\"./fast.html\">fast</li> <li class=\"tag\" href=\"./powerful.html\">powerful</li> </ul> </body> ''' html = etree . HTML ( HTML ) def test_regex_field (): title = ruia . RegexField ( re_select = '<div class=\"title\" href=\"(.*?)\">(.*?)</div>' ) assert title . extract ( html = HTML )[ 0 ] == '/' assert title . extract ( html = HTML )[ 1 ] == 'Ruia Documentation' tags = ruia . RegexField ( re_select = '<li class=\"tag\" href=\"(?P<href>.*?)\">(?P<text>.*?)</li>' , many = True ) result = tags . extract ( html = HTML ) assert isinstance ( result , list ) assert len ( result ) == 3 assert isinstance ( result [ 0 ], dict ) assert result [ 0 ][ 'href' ] == './easy.html'","title":"Example"},{"location":"en/apis/field.html#about-parameter-many","text":"Parameter many=False indicates if the field will extract one value or multiple values from HTML source code. For example, one Github Issue has many tags, We can use Item.get_items to get multiple values of tags, but that means an extra class definition. Parameter many aims to solve this problem. A field is default by many=False , that means, for TextField , AttrField and HtmlField , Field.extract(*, **) will always return a string, and RegexField will return a string or a list or dict, depending on whether there are groups in the regular expression. We can consider it with a 'singular number'. With many=True , each field will return a 'plural', that is, return a list.","title":"About Parameter many"},{"location":"en/apis/item.html","text":"Item item is mainly used to define data model and extract data from HTML source code. It has the following two methods: get_item : extract one data from HTML source code get_items : extract many data from HTML source code Core arguments get_item and get_items receives same arguments: - html: optional, HTML source code - url: optional, HTML href link - html_etree: optional, etree._Element object Usage From the arguments above, we can see that, there are three ways to feed Item object: from a web link, from HTML source code, or even from etree._Element object. import asyncio from ruia import AttrField , TextField , Item class HackerNewsItem ( Item ): target_item = TextField ( css_select = 'tr.athing' ) title = TextField ( css_select = 'a.storylink' ) url = AttrField ( css_select = 'a.storylink' , attr = 'href' ) async def clean_title ( self , value ): return value async def main (): async for item in HackerNewsItem . get_items ( url = \"https://news.ycombinator.com/\" ): print ( item . title , item . url ) if __name__ == '__main__' : items = asyncio . run ( main ()) Sometimes we may come across such a condition. When crawling github issues, we will find that there are several tags to a issue. Define TagItem as a standalone item is not that beautiful. It's time to focus on the many=True argument. Fields with many=True will return a list. from ruia import Item , TextField class GithiubIssueItem ( Item ): issue_id = TextField ( css_select = 'issue_id_class' ) title = TextField ( css_select = 'issue_title_class' ) tags = TextField ( css_select = 'tag_class' , many = True ) item = GithiubIssueItem . get_item ( html ) assert isinstance ( item . tags , list ) AttrField also has the argument many . How It Works? Inner, item class will change different kinds of inputs into etree._Element obejct, and then extract data. Meta class will help to get every property defined by Filed .","title":"Item"},{"location":"en/apis/item.html#item","text":"item is mainly used to define data model and extract data from HTML source code. It has the following two methods: get_item : extract one data from HTML source code get_items : extract many data from HTML source code","title":"Item"},{"location":"en/apis/item.html#core-arguments","text":"get_item and get_items receives same arguments: - html: optional, HTML source code - url: optional, HTML href link - html_etree: optional, etree._Element object","title":"Core arguments"},{"location":"en/apis/item.html#usage","text":"From the arguments above, we can see that, there are three ways to feed Item object: from a web link, from HTML source code, or even from etree._Element object. import asyncio from ruia import AttrField , TextField , Item class HackerNewsItem ( Item ): target_item = TextField ( css_select = 'tr.athing' ) title = TextField ( css_select = 'a.storylink' ) url = AttrField ( css_select = 'a.storylink' , attr = 'href' ) async def clean_title ( self , value ): return value async def main (): async for item in HackerNewsItem . get_items ( url = \"https://news.ycombinator.com/\" ): print ( item . title , item . url ) if __name__ == '__main__' : items = asyncio . run ( main ()) Sometimes we may come across such a condition. When crawling github issues, we will find that there are several tags to a issue. Define TagItem as a standalone item is not that beautiful. It's time to focus on the many=True argument. Fields with many=True will return a list. from ruia import Item , TextField class GithiubIssueItem ( Item ): issue_id = TextField ( css_select = 'issue_id_class' ) title = TextField ( css_select = 'issue_title_class' ) tags = TextField ( css_select = 'tag_class' , many = True ) item = GithiubIssueItem . get_item ( html ) assert isinstance ( item . tags , list ) AttrField also has the argument many .","title":"Usage"},{"location":"en/apis/item.html#how-it-works","text":"Inner, item class will change different kinds of inputs into etree._Element obejct, and then extract data. Meta class will help to get every property defined by Filed .","title":"How It Works?"},{"location":"en/apis/middleware.html","text":"Middleware Middleware is mainly used to process before request and process after response, such as listening request and response. Middleware().request : do some operations before request; Middleware().response : do some operations after response. Usage Note: The function should receive a special argument; The function return nothing. The arguments are listed in the following example: from ruia import Middleware middleware = Middleware () @middleware.request async def print_on_request ( request ): \"\"\" This function will be called before every request. request: an object of Request \"\"\" print ( \"request: print when a request is received\" ) @middleware.response async def print_on_response ( request , response ): \"\"\" This function will be called after every request. request: an object of Request response: an object of Response \"\"\" print ( \"response: print when a response is received\" ) How It Works? Middleware used decorators to implement the callback function, aims at writting middlewares easier for developers. Middleware().request_middleware and Middleware().response_middleware are two queues, stands for the user-defined functions.","title":"Middleware"},{"location":"en/apis/middleware.html#middleware","text":"Middleware is mainly used to process before request and process after response, such as listening request and response. Middleware().request : do some operations before request; Middleware().response : do some operations after response.","title":"Middleware"},{"location":"en/apis/middleware.html#usage","text":"Note: The function should receive a special argument; The function return nothing. The arguments are listed in the following example: from ruia import Middleware middleware = Middleware () @middleware.request async def print_on_request ( request ): \"\"\" This function will be called before every request. request: an object of Request \"\"\" print ( \"request: print when a request is received\" ) @middleware.response async def print_on_response ( request , response ): \"\"\" This function will be called after every request. request: an object of Request response: an object of Response \"\"\" print ( \"response: print when a response is received\" )","title":"Usage"},{"location":"en/apis/middleware.html#how-it-works","text":"Middleware used decorators to implement the callback function, aims at writting middlewares easier for developers. Middleware().request_middleware and Middleware().response_middleware are two queues, stands for the user-defined functions.","title":"How It Works?"},{"location":"en/apis/request.html","text":"Request Request is used for operating web requests. It returns a Response object. Methods: Request().fetch : request a web resource, it can be used standalone Request().fetch_callback : it is a core method for Spider class Core arguments url: the resource link method: request method, shoud be GET or `POST callback: callback function headers: request headers load_js: bool, if the target web page needs loading JS metadata: some data that need pass to next request request_config: the configure of the request request_session: aiohttp.ClientSession kwargs: other arguments for request Usage From the arguments above, we can see that Request can be used both associated with Spider and standalone. import asyncio from ruia import Request request = Request ( \"https://news.ycombinator.com/\" ) response = asyncio . get_event_loop () . run_until_complete ( request . fetch ()) # Output # [2018-07-25 11:23:42,620]-Request-INFO <GET: https://news.ycombinator.com/> # <Response url[text]: https://news.ycombinator.com/ status:200 metadata:{}> How It Works? Request class will send asynchronous http request by packaging aiohttp and pyppeteer .","title":"Request"},{"location":"en/apis/request.html#request","text":"Request is used for operating web requests. It returns a Response object. Methods: Request().fetch : request a web resource, it can be used standalone Request().fetch_callback : it is a core method for Spider class","title":"Request"},{"location":"en/apis/request.html#core-arguments","text":"url: the resource link method: request method, shoud be GET or `POST callback: callback function headers: request headers load_js: bool, if the target web page needs loading JS metadata: some data that need pass to next request request_config: the configure of the request request_session: aiohttp.ClientSession kwargs: other arguments for request","title":"Core arguments"},{"location":"en/apis/request.html#usage","text":"From the arguments above, we can see that Request can be used both associated with Spider and standalone. import asyncio from ruia import Request request = Request ( \"https://news.ycombinator.com/\" ) response = asyncio . get_event_loop () . run_until_complete ( request . fetch ()) # Output # [2018-07-25 11:23:42,620]-Request-INFO <GET: https://news.ycombinator.com/> # <Response url[text]: https://news.ycombinator.com/ status:200 metadata:{}>","title":"Usage"},{"location":"en/apis/request.html#how-it-works","text":"Request class will send asynchronous http request by packaging aiohttp and pyppeteer .","title":"How It Works?"},{"location":"en/apis/response.html","text":"Response Response is used to return a uniformed and friendly response object. Main properties: url: the href of resource metadata: some data from previous request html: HTML source code from website cookies: cookies of website history: the request history headers: response headers status: response status code","title":"Response"},{"location":"en/apis/response.html#response","text":"Response is used to return a uniformed and friendly response object. Main properties: url: the href of resource metadata: some data from previous request html: HTML source code from website cookies: cookies of website history: the request history headers: response headers status: response status code","title":"Response"},{"location":"en/apis/spider.html","text":"Spider Spider is the entrypoint of the crawler program. It combines Item , Middleware , Request and other models, to build a strong crawler for you. You should focus on the following two functions: Spider.start : the entrypoint parse : The first parse function, required for subclass of Spider Core arguments Spider.start arguments: after_start: a hook after starting the crawler before_stop: a hook before starting the crawler middleware: Middleware class, can be an object of Middleware() , or a list of Middleware() loop: event loop Usage import aiofiles from ruia import AttrField , TextField , Item , Spider class HackerNewsItem ( Item ): target_item = TextField ( css_select = 'tr.athing' ) title = TextField ( css_select = 'a.storylink' ) url = AttrField ( css_select = 'a.storylink' , attr = 'href' ) async def clean_title ( self , value ): return value class HackerNewsSpider ( Spider ): start_urls = [ 'https://news.ycombinator.com/news?p=1' , 'https://news.ycombinator.com/news?p=2' ] async def parse ( self , response ): async for item in HackerNewsItem . get_items ( html = response . html ): yield item async def process_item ( self , item : HackerNewsItem ): \"\"\"Ruia build-in method\"\"\" async with aiofiles . open ( './hacker_news.txt' , 'a' ) as f : await f . write ( str ( item . title ) + ' \\n ' ) if __name__ == '__main__' : HackerNewsSpider . start () Controlling the max number of concurrency Define an attribute concurrency at the subclass of Spider . Here is an example: import ruia class MySpider ( ruia . Spider ): start_urls = [ 'https://news.ycombinator.com' ] concurrency = 3 async def parse ( self , res ): pass How It Works? Spider will read links in start_urls , and maintains a asynchronous queue. The queue is a producer consumer model, and the loop will run until no more request functions.","title":"Spider"},{"location":"en/apis/spider.html#spider","text":"Spider is the entrypoint of the crawler program. It combines Item , Middleware , Request and other models, to build a strong crawler for you. You should focus on the following two functions: Spider.start : the entrypoint parse : The first parse function, required for subclass of Spider","title":"Spider"},{"location":"en/apis/spider.html#core-arguments","text":"Spider.start arguments: after_start: a hook after starting the crawler before_stop: a hook before starting the crawler middleware: Middleware class, can be an object of Middleware() , or a list of Middleware() loop: event loop","title":"Core arguments"},{"location":"en/apis/spider.html#usage","text":"import aiofiles from ruia import AttrField , TextField , Item , Spider class HackerNewsItem ( Item ): target_item = TextField ( css_select = 'tr.athing' ) title = TextField ( css_select = 'a.storylink' ) url = AttrField ( css_select = 'a.storylink' , attr = 'href' ) async def clean_title ( self , value ): return value class HackerNewsSpider ( Spider ): start_urls = [ 'https://news.ycombinator.com/news?p=1' , 'https://news.ycombinator.com/news?p=2' ] async def parse ( self , response ): async for item in HackerNewsItem . get_items ( html = response . html ): yield item async def process_item ( self , item : HackerNewsItem ): \"\"\"Ruia build-in method\"\"\" async with aiofiles . open ( './hacker_news.txt' , 'a' ) as f : await f . write ( str ( item . title ) + ' \\n ' ) if __name__ == '__main__' : HackerNewsSpider . start ()","title":"Usage"},{"location":"en/apis/spider.html#controlling-the-max-number-of-concurrency","text":"Define an attribute concurrency at the subclass of Spider . Here is an example: import ruia class MySpider ( ruia . Spider ): start_urls = [ 'https://news.ycombinator.com' ] concurrency = 3 async def parse ( self , res ): pass","title":"Controlling the max number of concurrency"},{"location":"en/apis/spider.html#how-it-works","text":"Spider will read links in start_urls , and maintains a asynchronous queue. The queue is a producer consumer model, and the loop will run until no more request functions.","title":"How It Works?"},{"location":"en/examples/project.html","text":"Start a Full Featured Ruia Project This essay will talk about how to create a full featured Ruia Spider Project. Comming soon...","title":"Full featured project"},{"location":"en/examples/project.html#start-a-full-featured-ruia-project","text":"This essay will talk about how to create a full featured Ruia Spider Project.","title":"Start a Full Featured Ruia Project"},{"location":"en/examples/project.html#comming-soon","text":"","title":"Comming soon..."},{"location":"en/topics/item_data_cleaning.html","text":"Item Data Cleaning If you visit other's blog, you may find that the titles of articles are often such a format: Ruia is a great framework | Ruia's blog . Open the inspector of browser, you will find such an element: < title > Ruia is a great framework | Ruia's blog </ title > The title element contains two parts: the actual title of this article and the site name of the blog. Now we just want to get the actual title Ruia is a great framework . We can write a statement in parse method, like: from ruia import Item , TextField class MyItem ( Item ): title = TextField ( css_select = 'title' ) async def parse ( self , response ): title = MyItem . get_item ( response . html ) . title title = title . split ( ' | ' )[ 0 ] with open ( 'data.txt' , mode = 'a' ) as file : file . writelines ([ title ]) It works well. However, in ruia, we want to separate the two processes: Data acquisition, for parsing HTML and create structured data; Data processing, for data persistence or some other operations. We provide a better way for data cleaning. from ruia import Item , TextField class MyItem ( Item ): title = TextField ( css_select = 'title' ) def clean_title ( self , value ): value = value . split ( ' | ' )[ 0 ] return value async def parse ( self , response ): title = MyItem . get_item ( response . html ) . title with open ( 'data.txt' , mode = 'a' ) as file : file . writelines ([ title ]) Now we get a better item. We just get the property title of item , like item.title , and we can get a pure title we want. ruia will automatically recognize methods starts with clean_ . If there's a field named the_field , then its corresponding data cleaning method is clean_the_field . Just add a prefix clean_ is okay. The default clean method of each field is just return the string itself. Before data cleaning, fields are all pure python strings (sometimes a list or a dict of pure python strings). If you want item.index to return a python integer, please define clean_index method to return int(value) . Separate the two processes makes our code more readable.","title":"Item Data Cleaning"},{"location":"en/topics/item_data_cleaning.html#item-data-cleaning","text":"If you visit other's blog, you may find that the titles of articles are often such a format: Ruia is a great framework | Ruia's blog . Open the inspector of browser, you will find such an element: < title > Ruia is a great framework | Ruia's blog </ title > The title element contains two parts: the actual title of this article and the site name of the blog. Now we just want to get the actual title Ruia is a great framework . We can write a statement in parse method, like: from ruia import Item , TextField class MyItem ( Item ): title = TextField ( css_select = 'title' ) async def parse ( self , response ): title = MyItem . get_item ( response . html ) . title title = title . split ( ' | ' )[ 0 ] with open ( 'data.txt' , mode = 'a' ) as file : file . writelines ([ title ]) It works well. However, in ruia, we want to separate the two processes: Data acquisition, for parsing HTML and create structured data; Data processing, for data persistence or some other operations. We provide a better way for data cleaning. from ruia import Item , TextField class MyItem ( Item ): title = TextField ( css_select = 'title' ) def clean_title ( self , value ): value = value . split ( ' | ' )[ 0 ] return value async def parse ( self , response ): title = MyItem . get_item ( response . html ) . title with open ( 'data.txt' , mode = 'a' ) as file : file . writelines ([ title ]) Now we get a better item. We just get the property title of item , like item.title , and we can get a pure title we want. ruia will automatically recognize methods starts with clean_ . If there's a field named the_field , then its corresponding data cleaning method is clean_the_field . Just add a prefix clean_ is okay. The default clean method of each field is just return the string itself. Before data cleaning, fields are all pure python strings (sometimes a list or a dict of pure python strings). If you want item.index to return a python integer, please define clean_index method to return int(value) . Separate the two processes makes our code more readable.","title":"Item Data Cleaning"},{"location":"en/topics/write_spiders_like_scrapy.html","text":"Write Spiders like Scrapy I am a user of scrapy myself. Scrapy is a great framework, which is a de facto standard of python crawlers for years. Ruia provides APIs like scrapy, for users to migrate crawlers from scrapy to ruia. If you like the Declarative Programming feature, but you know little about python async/await syntax, this essay is for you. For this example, we'll crawl Github Developer Documentation . # Target: https://developer.github.com/v3/ from ruia import * class CatalogueItem ( Item ): target_item = TextField ( css_select = '.sidebar-menu a' ) title = TextField ( css_select = 'a' ) link = AttrField ( css_select = 'a' , attr = 'href' ) async def clean_link ( self , value ): return f 'https://developer.github.com{value}' class PageItem ( Item ): content = HtmlField ( css_select = '.content' ) class GithubDeveloperSpider ( Spider ): start_urls = [ 'https://developer.github.com/v3/' ] async def parse ( self , response : Response ): catalogue = [] async for cat in CatalogueItem . get_items ( html = response . html ): catalogue . append ( cat ) for page in catalogue [: 6 ]: if '#' in page . link : continue yield Request ( url = page . link , metadata = { 'title' : page . title }, callback = self . parse_page ) async def parse_page ( self , response : Response ): item = await PageItem . get_item ( html = response . html ) title = response . metadata [ 'title' ] print ( title , len ( item . content )) See the GithubDeveloperSpider.parse method. After extracting titles and urls, it yield a request. About yield , you should learn from python documentation. Here we can regard it as sending a task to background process. Okay, now that we have already send the request to background process, we have loss the control of the request. Nothing serious, after the request finished, the response will send to its callback parameter. callback parameter should be a function, or something callable. In parse_page method, we accept the response. Then it comes with another problem: we have already get the page title from catalogue in method parse , but they are not in the context of parse_page . That's why we need a metadata argument. we put data into metadata in the previous method, and get data from it in the following method. Now, run the spider. output: Media Types 8652 Overview 38490 OAuth Authorizations API 66565 Other Authentication Methods 6651 Troubleshooting 2551 BTW, we sincerely recommend you to migrate your code to new APIs of ruia . ruia provides a better way to replace callback functions with coroutines. It provides more readability and is more flexible. We do not need callback and metadata now. Crawlers are more concise.","title":"Write Spiders like Scrapy"},{"location":"en/topics/write_spiders_like_scrapy.html#write-spiders-like-scrapy","text":"I am a user of scrapy myself. Scrapy is a great framework, which is a de facto standard of python crawlers for years. Ruia provides APIs like scrapy, for users to migrate crawlers from scrapy to ruia. If you like the Declarative Programming feature, but you know little about python async/await syntax, this essay is for you. For this example, we'll crawl Github Developer Documentation . # Target: https://developer.github.com/v3/ from ruia import * class CatalogueItem ( Item ): target_item = TextField ( css_select = '.sidebar-menu a' ) title = TextField ( css_select = 'a' ) link = AttrField ( css_select = 'a' , attr = 'href' ) async def clean_link ( self , value ): return f 'https://developer.github.com{value}' class PageItem ( Item ): content = HtmlField ( css_select = '.content' ) class GithubDeveloperSpider ( Spider ): start_urls = [ 'https://developer.github.com/v3/' ] async def parse ( self , response : Response ): catalogue = [] async for cat in CatalogueItem . get_items ( html = response . html ): catalogue . append ( cat ) for page in catalogue [: 6 ]: if '#' in page . link : continue yield Request ( url = page . link , metadata = { 'title' : page . title }, callback = self . parse_page ) async def parse_page ( self , response : Response ): item = await PageItem . get_item ( html = response . html ) title = response . metadata [ 'title' ] print ( title , len ( item . content )) See the GithubDeveloperSpider.parse method. After extracting titles and urls, it yield a request. About yield , you should learn from python documentation. Here we can regard it as sending a task to background process. Okay, now that we have already send the request to background process, we have loss the control of the request. Nothing serious, after the request finished, the response will send to its callback parameter. callback parameter should be a function, or something callable. In parse_page method, we accept the response. Then it comes with another problem: we have already get the page title from catalogue in method parse , but they are not in the context of parse_page . That's why we need a metadata argument. we put data into metadata in the previous method, and get data from it in the following method. Now, run the spider. output: Media Types 8652 Overview 38490 OAuth Authorizations API 66565 Other Authentication Methods 6651 Troubleshooting 2551 BTW, we sincerely recommend you to migrate your code to new APIs of ruia . ruia provides a better way to replace callback functions with coroutines. It provides more readability and is more flexible. We do not need callback and metadata now. Crawlers are more concise.","title":"Write Spiders like Scrapy"},{"location":"en/tutorials/installation.html","text":"Installation Ruia is based on Python 3.6+. Check your python version before continue. python --version Installation # For Linux & Mac pip install -U ruia [ uvloop ] # For Windows pip install -U ruia # Install from github for new features pip install git+https://github.com/howie6879/ruia","title":"2. Installation"},{"location":"en/tutorials/installation.html#installation","text":"Ruia is based on Python 3.6+. Check your python version before continue. python --version","title":"Installation"},{"location":"en/tutorials/installation.html#installation_1","text":"# For Linux & Mac pip install -U ruia [ uvloop ] # For Windows pip install -U ruia # Install from github for new features pip install git+https://github.com/howie6879/ruia","title":"Installation"},{"location":"en/tutorials/item.html","text":"Define Data Item Item and Fields First let's talk about what is Item . Item is an important concept in ruia . It defines what you want to get from HTML document. ruia.Item is used to get data from HTML document and save structured data. Define Item Here's an example of a simple Item. from ruia import Item , TextField , AttrField class PythonDocumentationItem ( Item ): title = TextField ( css_select = 'title' ) tutorial_link = AttrField ( xpath_select = \"//a[text()='Tutorial']\" , attr = 'href' ) Now let's reconstruct this Item. Analyze HTML document Supposing that we want to get the current python documentation version and its tutorial page link. We read the HTML source at https://docs.python.org/3/ . We find such two elements: < title > 3.7.2 Documentation </ title > < a class = \"biglink\" href = \"tutorial/index.html\" > Tutorial </ a > What we want to do: navigate to the element; extract data from element. Navigate to an element Ruia use selectors to navigate to the HTML element. As a crawler engineer, ruia believes that you have a full knowledge of at least one of CSS Selector and XPath Selector. For title element, because of his uniqueness, a simple CSS Selector is enough: css_select = 'title' For the Tuturial element, we have to use a XPath Selector to address it by it's text. xpath_select = \"//a[text()='Tutorial']\" Extract string from HTML element Now we have navigated to HTML elements. Time to extract string from it. Ruia use Field to extract data from HTML elements. For the title element, we want its text property. TextField is quite suitable for this purpose. from ruia import TextField title = TextField ( css_select = 'title' ) For the Tutorial element, we want its href property. AttrField is useful now: from ruia import AttrField tutorial_href = AttrField ( xpath_select = \"//a[text()='Tutorial'\" , attr = 'href' ) Combine fields to a item We have already told ruia how to find and extract data from HTML document. It's high time to combine them together as a structured data. from ruia import Item , TextField , AttrField class PythonDocumentationItem ( Item ): title = TextField ( css_select = 'title' ) tutorial_link = AttrField ( xpath_select = \"//a[text()='Tutorial']\" , attr = 'href' ) We inherit a new Item named HackerNewsItem from ruia.Item . Now, feed a HTML document to PythonDocumentationItem , it will extract the title and tutorial_link for us. Test this item We just defined an item. But will it perform just as what we want? Let's have a simple test. Ruia.Item has a convenient API. It's normal that is it can extract data from HTML document as a string. The magic is that it is also able to extract data from the given URL. import asyncio from ruia import Item , TextField , AttrField class PythonDocumentationItem ( Item ): title = TextField ( css_select = 'title' ) tutorial_link = AttrField ( xpath_select = \"//a[text()='Tutorial']\" , attr = 'href' ) async def main (): url = 'https://docs.python.org/3/' item = await PythonDocumentationItem . get_item ( url = url ) print ( item . title ) print ( item . tutorial_link ) if __name__ == '__main__' : # Python 3.7 required asyncio . run ( main ()) # For python 3.6 # loop = asyncio.new_event_loop() # loop.run_until_complete(main()) # Output: # [2019:01:21 18:19:02]-Request-INFO request: <GET: https://docs.python.org/3/> # 3.7.2 Documentation # tutorial/index.html We hope you have already know python asyncio library, and know its basic usage. If not, remember the following tips: Functions defined with async keyword are now named coroutine; Define coroutine with async keyword; Call coroutine with await keyword; Start coroutine use asyncio.run function like the example. Now focus on the screen output. The first line is the log of Ruia , and the following two lines are the data we want. Okay, we have already finished the construction of our first Item. Get Many Items from One Page Here's a HTML document. It's simple and readable. <!DOCTYPE html> < html lang = \"en\" > < head > < meta charset = \"UTF-8\" > < title > Title </ title > </ head > < body > < div class = \"container\" > < div class = \"movie\" >< a class = \"title\" > Movie 1 </ a >< span class = \"star\" > 3 </ span ></ div > < div class = \"movie\" >< a class = \"title\" > Movie 2 </ a >< span class = \"star\" > 5 </ span ></ div > < div class = \"movie\" >< a class = \"title\" > Movie 3 </ a >< span class = \"star\" > 2 </ span ></ div > < div class = \"movie\" >< a class = \"title\" > Movie 4 </ a >< span class = \"star\" > 1 </ span ></ div > < div class = \"movie\" >< a class = \"title\" > Movie 5 </ a >< span class = \"star\" > 5 </ span ></ div > </ div > </ body > </ html > It's a catalogue of movies. We want to get the name of movies and their stars. After analyzing document structure, we find that each movie is in a class div.movie . Then we can navigate to the container element by a CSS Selector: css_select='div.movie' . Then we can get our fields as before. ruia.Item has a convenient way to finish this task. If you define a target_item field to an Item, then it stands for the container. Here's an example. import asyncio from ruia import Item , AttrField , TextField HTML = \"\"\" <!DOCTYPE html> <html lang=\"en\"> <head> <meta charset=\"UTF-8\"> <title>Title</title> </head> <body> <div class=\"container\"> <div class=\"movie\"><a class=\"title\">Movie 1</a><span class=\"star\">3</span></div> <div class=\"movie\"><a class=\"title\">Movie 2</a><span class=\"star\">5</span></div> <div class=\"movie\"><a class=\"title\">Movie 3</a><span class=\"star\">2</span></div> <div class=\"movie\"><a class=\"title\">Movie 4</a><span class=\"star\">1</span></div> <div class=\"movie\"><a class=\"title\">Movie 5</a><span class=\"star\">5</span></div> </div> </body> </html> \"\"\" class MyItem ( Item ): target_item = TextField ( css_select = '.movie' ) title = TextField ( css_select = '.title' ) star = TextField ( css_select = '.star' ) async def clean_star ( self , value ): return int ( value ) async def main (): async for item in MyItem . get_items ( html = HTML ): print ( f 'Title={item.title}, Star={item.star}' ) if __name__ == '__main__' : asyncio . run ( main ()) # Python 3.7 required # For python 3.6 # loop = asyncio.new_event_loop() # loop.run_until_complete(main()) Previously, we call Item.get_item(html=HTML) to get an item. Here, we call Item.get_items(html=HTML) to get a list of items. The data cleaning methods still process a string, it has no difference. Output: Title=Movie 1, Star=3 Title=Movie 2, Star=5 Title=Movie 3, Star=2 Title=Movie 4, Star=1 Title=Movie 5, Star=5 Get Many Value by One Field Here is another example. Consider that we are now at a movie detail page. The HTML document shows like: <!DOCTYPE html> < html lang = \"en\" > < head > < meta charset = \"UTF-8\" > < title > Title </ title > </ head > < body > < div class = \"movie\" > < div class = \"title\" > Movie Title </ div > < div class = \"star\" > 5 </ div > < div class = \"tags\" > < div class = \"tag\" > Comedy </ div > < div class = \"tag\" > 2019 </ div > < div class = \"tag\" > China </ div > </ div > </ div > </ body > </ html > We want to get the title, star, and tags of this movie. Previously, we only pure strings from Field . Here we want to get a list a pure strings from tag field. ruia.Item provides an easy way, that is the many=True parameter of Fields . Here is the implementation: import asyncio from ruia import Item , TextField HTML = \"\"\" <!DOCTYPE html> <html lang=\"en\"> <head> <meta charset=\"UTF-8\"> <title>Title</title> </head> <body> <div class=\"movie\"> <div class=\"title\">Movie Title</div> <div class=\"star\">5</div> <div class=\"tags\"> <div class=\"tag\">Comedy</div> <div class=\"tag\">2019</div> <div class=\"tag\">China</div> </div> </div> </body> </html> \"\"\" class MyItem ( Item ): title = TextField ( css_select = '.title' ) star = TextField ( css_select = '.star' ) tags = TextField ( css_select = '.tag' , many = True ) async def clean_star ( self , value ): return int ( value ) async def main (): item = await MyItem . get_item ( html = HTML ) print ( 'Title: ' , item . title ) print ( 'Star: ' , item . star ) for tag in item . tags : print ( 'Tag: ' , tag ) if __name__ == '__main__' : asyncio . run ( main ()) # Python 3.7 required # For python 3.6 # loop = asyncio.new_event_loop() # loop.run_until_complete(main()) As we can see, tags field return a list of tags. All fields have this parameter. More Fields Ruia supports more fields than TextField and AttrField . However, they are the two fields that mostly used. Ruia also supports RegexField for extract data from HTML document directly by regular expression. It is only used for performance limitation, however, because of ruia's fast, we seldom meet performance limitation. There is a HtmlField to extract pure HTML source of a HTML element. Read Field API to get more information. Further Read the following essays for further learning. Data Cleaning","title":"3. Define Data Items"},{"location":"en/tutorials/item.html#define-data-item","text":"","title":"Define Data Item"},{"location":"en/tutorials/item.html#item-and-fields","text":"First let's talk about what is Item . Item is an important concept in ruia . It defines what you want to get from HTML document. ruia.Item is used to get data from HTML document and save structured data.","title":"Item and Fields"},{"location":"en/tutorials/item.html#define-item","text":"Here's an example of a simple Item. from ruia import Item , TextField , AttrField class PythonDocumentationItem ( Item ): title = TextField ( css_select = 'title' ) tutorial_link = AttrField ( xpath_select = \"//a[text()='Tutorial']\" , attr = 'href' ) Now let's reconstruct this Item.","title":"Define Item"},{"location":"en/tutorials/item.html#analyze-html-document","text":"Supposing that we want to get the current python documentation version and its tutorial page link. We read the HTML source at https://docs.python.org/3/ . We find such two elements: < title > 3.7.2 Documentation </ title > < a class = \"biglink\" href = \"tutorial/index.html\" > Tutorial </ a > What we want to do: navigate to the element; extract data from element.","title":"Analyze HTML document"},{"location":"en/tutorials/item.html#navigate-to-an-element","text":"Ruia use selectors to navigate to the HTML element. As a crawler engineer, ruia believes that you have a full knowledge of at least one of CSS Selector and XPath Selector. For title element, because of his uniqueness, a simple CSS Selector is enough: css_select = 'title' For the Tuturial element, we have to use a XPath Selector to address it by it's text. xpath_select = \"//a[text()='Tutorial']\"","title":"Navigate to an element"},{"location":"en/tutorials/item.html#extract-string-from-html-element","text":"Now we have navigated to HTML elements. Time to extract string from it. Ruia use Field to extract data from HTML elements. For the title element, we want its text property. TextField is quite suitable for this purpose. from ruia import TextField title = TextField ( css_select = 'title' ) For the Tutorial element, we want its href property. AttrField is useful now: from ruia import AttrField tutorial_href = AttrField ( xpath_select = \"//a[text()='Tutorial'\" , attr = 'href' )","title":"Extract string from HTML element"},{"location":"en/tutorials/item.html#combine-fields-to-a-item","text":"We have already told ruia how to find and extract data from HTML document. It's high time to combine them together as a structured data. from ruia import Item , TextField , AttrField class PythonDocumentationItem ( Item ): title = TextField ( css_select = 'title' ) tutorial_link = AttrField ( xpath_select = \"//a[text()='Tutorial']\" , attr = 'href' ) We inherit a new Item named HackerNewsItem from ruia.Item . Now, feed a HTML document to PythonDocumentationItem , it will extract the title and tutorial_link for us.","title":"Combine fields to a item"},{"location":"en/tutorials/item.html#test-this-item","text":"We just defined an item. But will it perform just as what we want? Let's have a simple test. Ruia.Item has a convenient API. It's normal that is it can extract data from HTML document as a string. The magic is that it is also able to extract data from the given URL. import asyncio from ruia import Item , TextField , AttrField class PythonDocumentationItem ( Item ): title = TextField ( css_select = 'title' ) tutorial_link = AttrField ( xpath_select = \"//a[text()='Tutorial']\" , attr = 'href' ) async def main (): url = 'https://docs.python.org/3/' item = await PythonDocumentationItem . get_item ( url = url ) print ( item . title ) print ( item . tutorial_link ) if __name__ == '__main__' : # Python 3.7 required asyncio . run ( main ()) # For python 3.6 # loop = asyncio.new_event_loop() # loop.run_until_complete(main()) # Output: # [2019:01:21 18:19:02]-Request-INFO request: <GET: https://docs.python.org/3/> # 3.7.2 Documentation # tutorial/index.html We hope you have already know python asyncio library, and know its basic usage. If not, remember the following tips: Functions defined with async keyword are now named coroutine; Define coroutine with async keyword; Call coroutine with await keyword; Start coroutine use asyncio.run function like the example. Now focus on the screen output. The first line is the log of Ruia , and the following two lines are the data we want. Okay, we have already finished the construction of our first Item.","title":"Test this item"},{"location":"en/tutorials/item.html#get-many-items-from-one-page","text":"Here's a HTML document. It's simple and readable. <!DOCTYPE html> < html lang = \"en\" > < head > < meta charset = \"UTF-8\" > < title > Title </ title > </ head > < body > < div class = \"container\" > < div class = \"movie\" >< a class = \"title\" > Movie 1 </ a >< span class = \"star\" > 3 </ span ></ div > < div class = \"movie\" >< a class = \"title\" > Movie 2 </ a >< span class = \"star\" > 5 </ span ></ div > < div class = \"movie\" >< a class = \"title\" > Movie 3 </ a >< span class = \"star\" > 2 </ span ></ div > < div class = \"movie\" >< a class = \"title\" > Movie 4 </ a >< span class = \"star\" > 1 </ span ></ div > < div class = \"movie\" >< a class = \"title\" > Movie 5 </ a >< span class = \"star\" > 5 </ span ></ div > </ div > </ body > </ html > It's a catalogue of movies. We want to get the name of movies and their stars. After analyzing document structure, we find that each movie is in a class div.movie . Then we can navigate to the container element by a CSS Selector: css_select='div.movie' . Then we can get our fields as before. ruia.Item has a convenient way to finish this task. If you define a target_item field to an Item, then it stands for the container. Here's an example. import asyncio from ruia import Item , AttrField , TextField HTML = \"\"\" <!DOCTYPE html> <html lang=\"en\"> <head> <meta charset=\"UTF-8\"> <title>Title</title> </head> <body> <div class=\"container\"> <div class=\"movie\"><a class=\"title\">Movie 1</a><span class=\"star\">3</span></div> <div class=\"movie\"><a class=\"title\">Movie 2</a><span class=\"star\">5</span></div> <div class=\"movie\"><a class=\"title\">Movie 3</a><span class=\"star\">2</span></div> <div class=\"movie\"><a class=\"title\">Movie 4</a><span class=\"star\">1</span></div> <div class=\"movie\"><a class=\"title\">Movie 5</a><span class=\"star\">5</span></div> </div> </body> </html> \"\"\" class MyItem ( Item ): target_item = TextField ( css_select = '.movie' ) title = TextField ( css_select = '.title' ) star = TextField ( css_select = '.star' ) async def clean_star ( self , value ): return int ( value ) async def main (): async for item in MyItem . get_items ( html = HTML ): print ( f 'Title={item.title}, Star={item.star}' ) if __name__ == '__main__' : asyncio . run ( main ()) # Python 3.7 required # For python 3.6 # loop = asyncio.new_event_loop() # loop.run_until_complete(main()) Previously, we call Item.get_item(html=HTML) to get an item. Here, we call Item.get_items(html=HTML) to get a list of items. The data cleaning methods still process a string, it has no difference. Output: Title=Movie 1, Star=3 Title=Movie 2, Star=5 Title=Movie 3, Star=2 Title=Movie 4, Star=1 Title=Movie 5, Star=5","title":"Get Many Items from One Page"},{"location":"en/tutorials/item.html#get-many-value-by-one-field","text":"Here is another example. Consider that we are now at a movie detail page. The HTML document shows like: <!DOCTYPE html> < html lang = \"en\" > < head > < meta charset = \"UTF-8\" > < title > Title </ title > </ head > < body > < div class = \"movie\" > < div class = \"title\" > Movie Title </ div > < div class = \"star\" > 5 </ div > < div class = \"tags\" > < div class = \"tag\" > Comedy </ div > < div class = \"tag\" > 2019 </ div > < div class = \"tag\" > China </ div > </ div > </ div > </ body > </ html > We want to get the title, star, and tags of this movie. Previously, we only pure strings from Field . Here we want to get a list a pure strings from tag field. ruia.Item provides an easy way, that is the many=True parameter of Fields . Here is the implementation: import asyncio from ruia import Item , TextField HTML = \"\"\" <!DOCTYPE html> <html lang=\"en\"> <head> <meta charset=\"UTF-8\"> <title>Title</title> </head> <body> <div class=\"movie\"> <div class=\"title\">Movie Title</div> <div class=\"star\">5</div> <div class=\"tags\"> <div class=\"tag\">Comedy</div> <div class=\"tag\">2019</div> <div class=\"tag\">China</div> </div> </div> </body> </html> \"\"\" class MyItem ( Item ): title = TextField ( css_select = '.title' ) star = TextField ( css_select = '.star' ) tags = TextField ( css_select = '.tag' , many = True ) async def clean_star ( self , value ): return int ( value ) async def main (): item = await MyItem . get_item ( html = HTML ) print ( 'Title: ' , item . title ) print ( 'Star: ' , item . star ) for tag in item . tags : print ( 'Tag: ' , tag ) if __name__ == '__main__' : asyncio . run ( main ()) # Python 3.7 required # For python 3.6 # loop = asyncio.new_event_loop() # loop.run_until_complete(main()) As we can see, tags field return a list of tags. All fields have this parameter.","title":"Get Many Value by One Field"},{"location":"en/tutorials/item.html#more-fields","text":"Ruia supports more fields than TextField and AttrField . However, they are the two fields that mostly used. Ruia also supports RegexField for extract data from HTML document directly by regular expression. It is only used for performance limitation, however, because of ruia's fast, we seldom meet performance limitation. There is a HtmlField to extract pure HTML source of a HTML element. Read Field API to get more information.","title":"More Fields"},{"location":"en/tutorials/item.html#further","text":"Read the following essays for further learning. Data Cleaning","title":"Further"},{"location":"en/tutorials/middleware.html","text":"","title":"6. Customize Middleware"},{"location":"en/tutorials/overview.html","text":"An Overview of Ruia Ruia is An asynchronous web scraping micro-framework, powered by asyncio and aiohttp , aims at making crawling url as convenient as possible. Write less, run faster is Ruia's philosophy. Ruia spider consists the following four parts: Ruia Part Is Required Description Data Items Required a collection of fields Spider Recommended a manager to make your spider stronger Middleware Optional used for processing request and response Plugin Optional used to enhance ruia functions","title":"1. Overview"},{"location":"en/tutorials/overview.html#an-overview-of-ruia","text":"Ruia is An asynchronous web scraping micro-framework, powered by asyncio and aiohttp , aims at making crawling url as convenient as possible. Write less, run faster is Ruia's philosophy. Ruia spider consists the following four parts: Ruia Part Is Required Description Data Items Required a collection of fields Spider Recommended a manager to make your spider stronger Middleware Optional used for processing request and response Plugin Optional used to enhance ruia functions","title":"An Overview of Ruia"},{"location":"en/tutorials/plugins.html","text":"How to Write a Plugins Plugins are used to package some common functions as a third-party model. Ruia allow developers to implement third-party extensions by Middleware class. In the previous section, we talked about Middleware . It is used to process before request and after response. Then, we implemeneted a function, that is to add User-Agent in request headers. Perhaps any crawler need such a function, to add User-Agent randomly, so, let's packaging this function as a third-party extension. Do it! Creating a project The project name is ruia-ua . Ruia is based on Python3.6+ , so is ruia-ua . Supposing that you're now in Python 3.6+ . # Install package management tool: pipenv pip install pipenv # Create project directory mkdir ruia-ua cd ruia-ua # Install virtual environment pipenv install # Install ruia pipenv install ruia # Install aiofiles pipenv install aiofiles # Create project directory in the project directory mkdir ruia_ua cd ruia_ua # Here's your implementation touch __init__.py Directory structure: ruia-ua \u251c\u2500\u2500 LICENSE # Open source license \u251c\u2500\u2500 Pipfile # pipenv management tools \u251c\u2500\u2500 Pipfile.lock \u251c\u2500\u2500 README.md \u251c\u2500\u2500 ruia_ua \u2502 \u251c\u2500\u2500 __init__.py # Main code of your plugin \u2502 \u2514\u2500\u2500 user_agents.txt # some random user_agents \u2514\u2500\u2500 setup.py First plugin user_agents.txt contains all kinds of UA , then we only need to use Middleware of ruia to add a random User-Agent before every request. Here is one implementation: import os import random import aiofiles from ruia import Middleware __version__ = \"0.0.1\" async def get_random_user_agent () -> str : \"\"\" Get a random user agent string. :return: Random user agent string. \"\"\" USER_AGENT = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.95 Safari/537.36' return random . choice ( await _get_data ( './user_agents.txt' , USER_AGENT )) async def _get_data ( filename : str , default : str ) -> list : \"\"\" Get data from all user_agents :param filename: filename :param default: default value :return: data \"\"\" root_folder = os . path . dirname ( __file__ ) user_agents_file = os . path . join ( root_folder , filename ) try : async with aiofiles . open ( user_agents_file , mode = 'r' ) as f : data = [ _ . strip () for _ in await f . readlines ()] except : data = [ default ] return data middleware = Middleware () @middleware.request async def add_random_ua ( request ): ua = await get_random_user_agent () if request . headers : request . headers . update ({ 'User-Agent' : ua }) else : request . headers = { 'User-Agent' : ua } Now it's high time to upload ruia-ua to community, then all other ruia users are able to use your third-party extension. Sounds great! Usage All crawlers can use ruia-ua to add User-Agent automatically. pip install ruia - ua Here is an example: from ruia import AttrField , TextField , Item , Spider from ruia_ua import middleware class HackerNewsItem ( Item ): target_item = TextField ( css_select = 'tr.athing' ) title = TextField ( css_select = 'a.storylink' ) url = AttrField ( css_select = 'a.storylink' , attr = 'href' ) async def clean_title ( self , value ): return value class HackerNewsSpider ( Spider ): start_urls = [ 'https://news.ycombinator.com/news?p=1' , 'https://news.ycombinator.com/news?p=2' ] concurrency = 10 async def parse ( self , response ): async for item in HackerNewsItem . get_items ( html = response . html ): print ( item . title ) if __name__ == '__main__' : HackerNewsSpider . start ( middleware = middleware ) The implementations of third-party plugins will make developing crawlers easier! Ruia do want your developing and uploading your own third-party plugins!","title":"7. Write a Plugins"},{"location":"en/tutorials/plugins.html#how-to-write-a-plugins","text":"Plugins are used to package some common functions as a third-party model. Ruia allow developers to implement third-party extensions by Middleware class. In the previous section, we talked about Middleware . It is used to process before request and after response. Then, we implemeneted a function, that is to add User-Agent in request headers. Perhaps any crawler need such a function, to add User-Agent randomly, so, let's packaging this function as a third-party extension. Do it!","title":"How to Write a Plugins"},{"location":"en/tutorials/plugins.html#creating-a-project","text":"The project name is ruia-ua . Ruia is based on Python3.6+ , so is ruia-ua . Supposing that you're now in Python 3.6+ . # Install package management tool: pipenv pip install pipenv # Create project directory mkdir ruia-ua cd ruia-ua # Install virtual environment pipenv install # Install ruia pipenv install ruia # Install aiofiles pipenv install aiofiles # Create project directory in the project directory mkdir ruia_ua cd ruia_ua # Here's your implementation touch __init__.py Directory structure: ruia-ua \u251c\u2500\u2500 LICENSE # Open source license \u251c\u2500\u2500 Pipfile # pipenv management tools \u251c\u2500\u2500 Pipfile.lock \u251c\u2500\u2500 README.md \u251c\u2500\u2500 ruia_ua \u2502 \u251c\u2500\u2500 __init__.py # Main code of your plugin \u2502 \u2514\u2500\u2500 user_agents.txt # some random user_agents \u2514\u2500\u2500 setup.py","title":"Creating a project"},{"location":"en/tutorials/plugins.html#first-plugin","text":"user_agents.txt contains all kinds of UA , then we only need to use Middleware of ruia to add a random User-Agent before every request. Here is one implementation: import os import random import aiofiles from ruia import Middleware __version__ = \"0.0.1\" async def get_random_user_agent () -> str : \"\"\" Get a random user agent string. :return: Random user agent string. \"\"\" USER_AGENT = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.95 Safari/537.36' return random . choice ( await _get_data ( './user_agents.txt' , USER_AGENT )) async def _get_data ( filename : str , default : str ) -> list : \"\"\" Get data from all user_agents :param filename: filename :param default: default value :return: data \"\"\" root_folder = os . path . dirname ( __file__ ) user_agents_file = os . path . join ( root_folder , filename ) try : async with aiofiles . open ( user_agents_file , mode = 'r' ) as f : data = [ _ . strip () for _ in await f . readlines ()] except : data = [ default ] return data middleware = Middleware () @middleware.request async def add_random_ua ( request ): ua = await get_random_user_agent () if request . headers : request . headers . update ({ 'User-Agent' : ua }) else : request . headers = { 'User-Agent' : ua } Now it's high time to upload ruia-ua to community, then all other ruia users are able to use your third-party extension. Sounds great!","title":"First plugin"},{"location":"en/tutorials/plugins.html#usage","text":"All crawlers can use ruia-ua to add User-Agent automatically. pip install ruia - ua Here is an example: from ruia import AttrField , TextField , Item , Spider from ruia_ua import middleware class HackerNewsItem ( Item ): target_item = TextField ( css_select = 'tr.athing' ) title = TextField ( css_select = 'a.storylink' ) url = AttrField ( css_select = 'a.storylink' , attr = 'href' ) async def clean_title ( self , value ): return value class HackerNewsSpider ( Spider ): start_urls = [ 'https://news.ycombinator.com/news?p=1' , 'https://news.ycombinator.com/news?p=2' ] concurrency = 10 async def parse ( self , response ): async for item in HackerNewsItem . get_items ( html = response . html ): print ( item . title ) if __name__ == '__main__' : HackerNewsSpider . start ( middleware = middleware ) The implementations of third-party plugins will make developing crawlers easier! Ruia do want your developing and uploading your own third-party plugins!","title":"Usage"},{"location":"en/tutorials/request.html","text":"Request & Response Ruia provides friendly and convenient Request and Response APis. Here is an example: import asyncio from ruia import Request async def request_example (): url = 'http://www.httpbin.org/get' params = { 'name' : 'ruia' , } headers = { 'User-Agent' : 'Python3.6' , } request = Request ( url = url , method = 'GET' , params = params , headers = headers ) response = await request . fetch () json_result = await response . json () assert json_result [ 'args' ][ 'name' ] == 'ruia' assert json_result [ 'headers' ][ 'User-Agent' ] == 'Python3.6' if __name__ == '__main__' : asyncio . get_event_loop () . run_until_complete ( request_example ()) We define a request by class Request . Then, we call await request.fetch() to get it's response. Note ruia.Request provides asynchronous methods, be sure to use it in an asynchronous function with async statement, and get it's respones with await statement. For full usage of response and request, see Request API and Response API","title":"5. Request & Response"},{"location":"en/tutorials/request.html#request-response","text":"Ruia provides friendly and convenient Request and Response APis. Here is an example: import asyncio from ruia import Request async def request_example (): url = 'http://www.httpbin.org/get' params = { 'name' : 'ruia' , } headers = { 'User-Agent' : 'Python3.6' , } request = Request ( url = url , method = 'GET' , params = params , headers = headers ) response = await request . fetch () json_result = await response . json () assert json_result [ 'args' ][ 'name' ] == 'ruia' assert json_result [ 'headers' ][ 'User-Agent' ] == 'Python3.6' if __name__ == '__main__' : asyncio . get_event_loop () . run_until_complete ( request_example ()) We define a request by class Request . Then, we call await request.fetch() to get it's response. Note ruia.Request provides asynchronous methods, be sure to use it in an asynchronous function with async statement, and get it's respones with await statement. For full usage of response and request, see Request API and Response API","title":"Request &amp; Response"},{"location":"en/tutorials/spider.html","text":"Spider Control ruia.Spider is used to control the whole spider, it provides the following functions: Normalize your code Maintain a event loop Manage requests and responses Concurrency control Manage middlewares and plugins Although it works well, to use only ruia.Item to create a spider, ruia recommend to use ruia.Spider to implement a stronger spider. Normalize your code ruia.Spider requires a class property start_urls as the entry point of a spider. Inner, ruia will iterate start_urls , and send a request to server for each request. After receiving server response, ruia will call spider.parse(response) , and this is the main part of your spider. Here's a simple parse example, to simply save response fields to a text file. We only have to define start_urls , and implement a parst method. import aiofiles from ruia import Spider , Item , TextField , AttrField class HackerNewsItem ( Item ): target_item = TextField ( css_select = 'tr.athing' ) title = TextField ( css_select = 'a.storylink' ) url = AttrField ( css_select = 'a.storylink' , attr = 'href' ) class HackerNewsSpider ( Spider ): start_urls = [ f 'https://news.ycombinator.com/news?p={index}' for index in range ( 3 )] async def parse ( self , response ): async for item in HackerNewsItem . get_items ( html = response . html ): yield item async def process_item ( self , item : HackerNewsItem ): \"\"\"Ruia build-in method\"\"\" async with aiofiles . open ( './hacker_news.txt' , 'a' ) as f : await f . write ( str ( item . title ) + ' \\n ' ) aiofiles is a third-party library to operate files in asynchronous way. It provides APIs the same as python standard open function. Now, we have written a spider, and time to start crawling. import aiofiles from ruia import Spider , Item , TextField , AttrField class HackerNewsItem ( Item ): target_item = TextField ( css_select = 'tr.athing' ) title = TextField ( css_select = 'a.storylink' ) url = AttrField ( css_select = 'a.storylink' , attr = 'href' ) class HackerNewsSpider ( Spider ): start_urls = [ f 'https://news.ycombinator.com/news?p={index}' for index in range ( 3 )] async def parse ( self , response ): async for item in HackerNewsItem . get_items ( html = response . html ): yield item async def process_item ( self , item : HackerNewsItem ): \"\"\"Ruia build-in method\"\"\" async with aiofiles . open ( './hacker_news.txt' , 'a' ) as f : await f . write ( str ( item . title ) + ' \\n ' ) if __name__ == '__main__' : HackerNewsSpider . start () Done. Now your code is more readable and maintainable. Send Further Requests I don't think that just crawling the catalogue of news satisfied you. Next we will crawl news itself. Hacker news gathers news from many websites, it's not easy to parse each article of it. For this example, we'll crawl Github Developer Documentation . If you are a user of scrapy , perhaps you'd like this essay for migration: Write Spiders like Scrapy . Ruia provides a better way to send further requests by new asynchronous syntax async/await. It provides more readability and is more flexible. In any parse method, just yield a coroutine, and the coroutine will be processed by ruia . Here is a simple pseudo-code. from ruia import Spider class MySpider ( Spider ): async def parse ( self , response ): next_response = await self . request ( f '{response.url}/next' ) yield self . parse_next_page ( next_response , metadata = 'nothing' ) async def parse_next_page ( self , response , metadata ): print ( response . html ) It works well, except you want to yield many coroutines in a for loop. Look at the following pseudo-code: from ruia import Spider class MySpider ( Spider ): async def parse ( self , response ): for i in range ( 10 ): response = await self . request ( f 'https://some.site/{i}' ) yield self . parse_next ( response ) async def parse_next ( self , response ): print ( response . html ) You will find the requests in the for loop runs in a synchronous way! Oh, awful. To solve this problem, ruia provides a multiple_request method. Here is an example for Github Developer Documentation. # Target: https://developer.github.com/v3/ from ruia import * class CatalogueItem ( Item ): target_item = TextField ( css_select = '.sidebar-menu a' ) title = TextField ( css_select = 'a' ) link = AttrField ( css_select = 'a' , attr = 'href' ) async def clean_link ( self , value ): return f 'https://developer.github.com{value}' class PageItem ( Item ): content = HtmlField ( css_select = '.content' ) class GithubDeveloperSpider ( Spider ): start_urls = [ 'https://developer.github.com/v3/' ] concurrency = 5 async def parse ( self , response : Response ): catalogue = [] async for cat in CatalogueItem . get_items ( html = response . html ): if '#' in cat . link : continue catalogue . append ( cat ) urls = [ page . link for page in catalogue ][: 10 ] async for response in self . multiple_request ( urls , is_gather = True ): title = catalogue [ response . index ] . title yield self . parse_page ( response , title ) async def parse_page ( self , response , title ): item = await PageItem . get_item ( html = response . html ) print ( title , len ( item . content )) if __name__ == '__main__' : GithubDeveloperSpider . start () Our crawler starts with start_urls and parse method as usual. We get a list of urls. Then we call self.multiple_request method to send further requests. multiple_request(urls, **kwargs) method requires a positional argument urls . It is a list of string. It also accept any other arguments like ruia.Request . Pay attention to use async for statement. multiple_request method returns an asynchronous generator. It yields responses. You may want to use enumerate to get the index of responses like this: async def parse ( self , response ): urls = [ f 'https://site.com/{page}' for page in range ( 10 )] async for response in enumerate ( self . multiple_request ( urls )): pass Then you will get an Exception, telling you that enumerate cannot process asynchronous generator. ruia provides a property for every response object: response.index . It is useful when you want to pass some context to the next parsing method. The order of responses currently is just the same as urls, but it's an unstable feature. Use response.index to get its position. multiple_request has another argument is_gather , which indicates whether ruia should run the requests together or not. If is_gather=True , then the requests will run together. If not, the requests will run one by one. is_gather=True is usually better, except one condition: we have a catalogue contains 1000 pages. If we use gather=True , we will get the response after 1000 requests. It may take too long before parsing. Concurrency Control Let's repeat the Github Developer spider. # Target: https://developer.github.com/v3/ from ruia import * class CatalogueItem ( Item ): target_item = TextField ( css_select = '.sidebar-menu a' ) title = TextField ( css_select = 'a' ) link = AttrField ( css_select = 'a' , attr = 'href' ) async def clean_link ( self , value ): return f 'https://developer.github.com{value}' class PageItem ( Item ): content = HtmlField ( css_select = '.content' ) class GithubDeveloperSpider ( Spider ): start_urls = [ 'https://developer.github.com/v3/' ] concurrency = 5 async def parse ( self , response : Response ): catalogue = [] async for cat in CatalogueItem . get_items ( html = response . html ): catalogue . append ( cat ) for page in catalogue [: 20 ]: if '#' in page . link : continue yield Request ( url = page . link , metadata = { 'title' : page . title }, callback = self . parse_page ) async def parse_page ( self , response : Response ): item = await PageItem . get_item ( html = response . html ) title = response . metadata [ 'title' ] print ( title , len ( item . content )) if __name__ == '__main__' : GithubDeveloperSpider . start () This time, there's a line added: concurrency = 5 Here's a brief introduction about concurrency. Some websites are friendly to crawlers, while some are not. If you visit a website too frequently, you will be banned from the server. Besides, to be a good crawler, we should protect the server, rather than making it crash. Not every server can burden a huge spider. To protect both, we have to control our concurrency. Concurrency means the connection numbers in a time. In this case, we set it to 5. Let's have a short look on the log. Output: [2019:01:23 00:01:59]-ruia-INFO spider : Spider started! [2019:01:23 00:01:59]-ruia-WARNINGspider : ruia tried to use loop.add_signal_handler but it is not implemented on this platform. [2019:01:23 00:01:59]-ruia-WARNINGspider : ruia tried to use loop.add_signal_handler but it is not implemented on this platform. [2019:01:23 00:01:59]-Request-INFO request: <GET: https://developer.github.com/v3/> [2019:01:23 00:02:00]-Request-INFO request: <GET: https://developer.github.com/v3/> [2019:01:23 00:02:00]-Request-INFO request: <GET: https://developer.github.com/v3/media/> [2019:01:23 00:02:00]-Request-INFO request: <GET: https://developer.github.com/v3/oauth_authorizations/> [2019:01:23 00:02:00]-Request-INFO request: <GET: https://developer.github.com/v3/auth/> [2019:01:23 00:02:00]-Request-INFO request: <GET: https://developer.github.com/v3/troubleshooting/> [2019:01:23 00:02:01]-Request-INFO request: <GET: https://developer.github.com/v3/previews/> Overview 38490 [2019:01:23 00:02:02]-Request-INFO request: <GET: https://developer.github.com/v3/versions/> OAuth Authorizations API 66565 [2019:01:23 00:02:02]-Request-INFO request: <GET: https://developer.github.com/v3/activity/> Media Types 8652 [2019:01:23 00:02:02]-Request-INFO request: <GET: https://developer.github.com/v3/activity/events/> Troubleshooting 2551 [2019:01:23 00:02:02]-Request-INFO request: <GET: https://developer.github.com/v3/activity/events/types/> API Previews 19537 [2019:01:23 00:02:02]-Request-INFO request: <GET: https://developer.github.com/v3/activity/feeds/> Other Authentication Methods 6651 [2019:01:23 00:02:03]-Request-INFO request: <GET: https://developer.github.com/v3/activity/notifications/> Versions 1344 Feeds 14090 [2019:01:23 00:02:03]-Request-INFO request: <GET: https://developer.github.com/v3/activity/starring/> Activity 2178 [2019:01:23 00:02:04]-Request-INFO request: <GET: https://developer.github.com/v3/activity/watching/> [2019:01:23 00:02:05]-Request-INFO request: <GET: https://developer.github.com/v3/checks/> Events 11844 Starring 55228 [2019:01:23 00:02:05]-Request-INFO request: <GET: https://developer.github.com/v3/checks/runs/> [2019:01:23 00:02:05]-Request-INFO request: <GET: https://developer.github.com/v3/checks/suites/> Event Types & Payloads 1225037 Notifications 65679 Watching 35775 Checks 7379 Check Runs 116607 [2019:01:23 00:02:06]-ruia-INFO spider : Stopping spider: ruia Check Suites 115330 [2019:01:23 00:02:06]-ruia-INFO spider : Total requests: 18 [2019:01:23 00:02:06]-ruia-INFO spider : Time usage: 0:00:07.342048 [2019:01:23 00:02:06]-ruia-INFO spider : Spider finished! Focus on the first several lines. [2019:01:23 00:01:54]-Request-INFO request: <GET: https://developer.github.com/v3/> [2019:01:23 00:02:00]-Request-INFO request: <GET: https://developer.github.com/v3/> [2019:01:23 00:02:00]-Request-INFO request: <GET: https://developer.github.com/v3/media/> [2019:01:23 00:02:00]-Request-INFO request: <GET: https://developer.github.com/v3/oauth_authorizations/> [2019:01:23 00:02:00]-Request-INFO request: <GET: https://developer.github.com/v3/auth/> [2019:01:23 00:02:00]-Request-INFO request: <GET: https://developer.github.com/v3/troubleshooting/> [2019:01:23 00:02:05]-Request-INFO request: <GET: https://developer.github.com/v3/previews/> Overview 38490 [2019:01:23 00:02:07]-Request-INFO request: <GET: https://developer.github.com/v3/versions/> OAuth Authorizations API 66565 The first request is at our requesting the catalogue page. Then, our spider send 5 requests at almost same time, at [00:02:00] . 5 seconds later, at [00:02:05] , our spider receives a response, and then sent another request. The response was parsed immediately. 2 seconds later, at [00:02:07] , our spider receives another response, and sent another request. Then, it parsed the response immediately. That is to say, at any time, there are 5 connections between spider and server. That is concurrency control. Hey, notice that our spider sent 5 requests at same time! Thanks to python's asyncio library, we can write asynchronous crawler easier and faster. Coroutines runs faster than multi-threadings. Use Middleware Ruia provides mainly two ways to enhance itself. Firstly let's talk about middlewares. Middlewares are used to process a request before it's sending and to process a response after it's receiving In a word, it is something between your spider and server. Here is a simple middleware named ruia-ua , it is used to automatically add random User-Agent to your requests. Firstly, install ruia-ua . pip install ruia-ua Then, add it to your spider. from ruia import AttrField , TextField , Item , Spider from ruia_ua import middleware class HackerNewsItem ( Item ): target_item = TextField ( css_select = 'tr.athing' ) title = TextField ( css_select = 'a.storylink' ) url = AttrField ( css_select = 'a.storylink' , attr = 'href' ) async def clean_title ( self , value ): return value class HackerNewsSpider ( Spider ): start_urls = [ 'https://news.ycombinator.com/news?p=1' , 'https://news.ycombinator.com/news?p=2' ] concurrency = 10 async def parse ( self , res ): async for item in HackerNewsItem . get_items ( html = res . html ): print ( item . title ) if __name__ == '__main__' : HackerNewsSpider . start ( middleware = middleware ) ruia.Spider has an argument middleware . It receives a list or a single middleware. Use Plugin If you want better control of your spider, try to use some plugins. ruia-pyppeteer is a ruia plugin used for loading JavaScript. Firstly, install ruia-pyppeteer . pip install ruia_pyppeteer # New features pip install git+https://github.com/ruia-plugins/ruia-pyppeteer Note When you use load_js, it will download a recent version of Chromium (~100MB). This only happens once. Here is a simple example to show how to load JavaScript. import asyncio from ruia_pyppeteer import PyppeteerRequest as Request request = Request ( \"https://www.jianshu.com/\" , load_js = True ) response = asyncio . get_event_loop () . run_until_complete ( request . fetch ()) print ( response . html ) Here is an example to use it in your spider: from ruia import AttrField , TextField , Item from ruia_pyppeteer import PyppeteerSpider as Spider from ruia_pyppeteer import PyppeteerRequest as Request class JianshuItem ( Item ): target_item = TextField ( css_select = 'ul.list>li' ) author_name = TextField ( css_select = 'a.name' ) author_url = AttrField ( attr = 'href' , css_select = 'a.name' ) async def clean_author_url ( self , author_url ): return f \"https://www.jianshu.com{author_url}\" class JianshuSpider ( Spider ): start_urls = [ 'https://www.jianshu.com/' ] concurrency = 10 # Load js on the first request load_js = True async def parse ( self , response ): async for item in JianshuItem . get_items ( html = response . html ): # Loading js by using PyppeteerRequest yield Request ( url = item . author_url , load_js = self . load_js , callback = self . parse_item ) async def parse_item ( self , response ): print ( response ) if __name__ == '__main__' : JianshuSpider . start ()","title":"4. Spider Control"},{"location":"en/tutorials/spider.html#spider-control","text":"ruia.Spider is used to control the whole spider, it provides the following functions: Normalize your code Maintain a event loop Manage requests and responses Concurrency control Manage middlewares and plugins Although it works well, to use only ruia.Item to create a spider, ruia recommend to use ruia.Spider to implement a stronger spider.","title":"Spider Control"},{"location":"en/tutorials/spider.html#normalize-your-code","text":"ruia.Spider requires a class property start_urls as the entry point of a spider. Inner, ruia will iterate start_urls , and send a request to server for each request. After receiving server response, ruia will call spider.parse(response) , and this is the main part of your spider. Here's a simple parse example, to simply save response fields to a text file. We only have to define start_urls , and implement a parst method. import aiofiles from ruia import Spider , Item , TextField , AttrField class HackerNewsItem ( Item ): target_item = TextField ( css_select = 'tr.athing' ) title = TextField ( css_select = 'a.storylink' ) url = AttrField ( css_select = 'a.storylink' , attr = 'href' ) class HackerNewsSpider ( Spider ): start_urls = [ f 'https://news.ycombinator.com/news?p={index}' for index in range ( 3 )] async def parse ( self , response ): async for item in HackerNewsItem . get_items ( html = response . html ): yield item async def process_item ( self , item : HackerNewsItem ): \"\"\"Ruia build-in method\"\"\" async with aiofiles . open ( './hacker_news.txt' , 'a' ) as f : await f . write ( str ( item . title ) + ' \\n ' ) aiofiles is a third-party library to operate files in asynchronous way. It provides APIs the same as python standard open function. Now, we have written a spider, and time to start crawling. import aiofiles from ruia import Spider , Item , TextField , AttrField class HackerNewsItem ( Item ): target_item = TextField ( css_select = 'tr.athing' ) title = TextField ( css_select = 'a.storylink' ) url = AttrField ( css_select = 'a.storylink' , attr = 'href' ) class HackerNewsSpider ( Spider ): start_urls = [ f 'https://news.ycombinator.com/news?p={index}' for index in range ( 3 )] async def parse ( self , response ): async for item in HackerNewsItem . get_items ( html = response . html ): yield item async def process_item ( self , item : HackerNewsItem ): \"\"\"Ruia build-in method\"\"\" async with aiofiles . open ( './hacker_news.txt' , 'a' ) as f : await f . write ( str ( item . title ) + ' \\n ' ) if __name__ == '__main__' : HackerNewsSpider . start () Done. Now your code is more readable and maintainable.","title":"Normalize your code"},{"location":"en/tutorials/spider.html#send-further-requests","text":"I don't think that just crawling the catalogue of news satisfied you. Next we will crawl news itself. Hacker news gathers news from many websites, it's not easy to parse each article of it. For this example, we'll crawl Github Developer Documentation . If you are a user of scrapy , perhaps you'd like this essay for migration: Write Spiders like Scrapy . Ruia provides a better way to send further requests by new asynchronous syntax async/await. It provides more readability and is more flexible. In any parse method, just yield a coroutine, and the coroutine will be processed by ruia . Here is a simple pseudo-code. from ruia import Spider class MySpider ( Spider ): async def parse ( self , response ): next_response = await self . request ( f '{response.url}/next' ) yield self . parse_next_page ( next_response , metadata = 'nothing' ) async def parse_next_page ( self , response , metadata ): print ( response . html ) It works well, except you want to yield many coroutines in a for loop. Look at the following pseudo-code: from ruia import Spider class MySpider ( Spider ): async def parse ( self , response ): for i in range ( 10 ): response = await self . request ( f 'https://some.site/{i}' ) yield self . parse_next ( response ) async def parse_next ( self , response ): print ( response . html ) You will find the requests in the for loop runs in a synchronous way! Oh, awful. To solve this problem, ruia provides a multiple_request method. Here is an example for Github Developer Documentation. # Target: https://developer.github.com/v3/ from ruia import * class CatalogueItem ( Item ): target_item = TextField ( css_select = '.sidebar-menu a' ) title = TextField ( css_select = 'a' ) link = AttrField ( css_select = 'a' , attr = 'href' ) async def clean_link ( self , value ): return f 'https://developer.github.com{value}' class PageItem ( Item ): content = HtmlField ( css_select = '.content' ) class GithubDeveloperSpider ( Spider ): start_urls = [ 'https://developer.github.com/v3/' ] concurrency = 5 async def parse ( self , response : Response ): catalogue = [] async for cat in CatalogueItem . get_items ( html = response . html ): if '#' in cat . link : continue catalogue . append ( cat ) urls = [ page . link for page in catalogue ][: 10 ] async for response in self . multiple_request ( urls , is_gather = True ): title = catalogue [ response . index ] . title yield self . parse_page ( response , title ) async def parse_page ( self , response , title ): item = await PageItem . get_item ( html = response . html ) print ( title , len ( item . content )) if __name__ == '__main__' : GithubDeveloperSpider . start () Our crawler starts with start_urls and parse method as usual. We get a list of urls. Then we call self.multiple_request method to send further requests. multiple_request(urls, **kwargs) method requires a positional argument urls . It is a list of string. It also accept any other arguments like ruia.Request . Pay attention to use async for statement. multiple_request method returns an asynchronous generator. It yields responses. You may want to use enumerate to get the index of responses like this: async def parse ( self , response ): urls = [ f 'https://site.com/{page}' for page in range ( 10 )] async for response in enumerate ( self . multiple_request ( urls )): pass Then you will get an Exception, telling you that enumerate cannot process asynchronous generator. ruia provides a property for every response object: response.index . It is useful when you want to pass some context to the next parsing method. The order of responses currently is just the same as urls, but it's an unstable feature. Use response.index to get its position. multiple_request has another argument is_gather , which indicates whether ruia should run the requests together or not. If is_gather=True , then the requests will run together. If not, the requests will run one by one. is_gather=True is usually better, except one condition: we have a catalogue contains 1000 pages. If we use gather=True , we will get the response after 1000 requests. It may take too long before parsing.","title":"Send Further Requests"},{"location":"en/tutorials/spider.html#concurrency-control","text":"Let's repeat the Github Developer spider. # Target: https://developer.github.com/v3/ from ruia import * class CatalogueItem ( Item ): target_item = TextField ( css_select = '.sidebar-menu a' ) title = TextField ( css_select = 'a' ) link = AttrField ( css_select = 'a' , attr = 'href' ) async def clean_link ( self , value ): return f 'https://developer.github.com{value}' class PageItem ( Item ): content = HtmlField ( css_select = '.content' ) class GithubDeveloperSpider ( Spider ): start_urls = [ 'https://developer.github.com/v3/' ] concurrency = 5 async def parse ( self , response : Response ): catalogue = [] async for cat in CatalogueItem . get_items ( html = response . html ): catalogue . append ( cat ) for page in catalogue [: 20 ]: if '#' in page . link : continue yield Request ( url = page . link , metadata = { 'title' : page . title }, callback = self . parse_page ) async def parse_page ( self , response : Response ): item = await PageItem . get_item ( html = response . html ) title = response . metadata [ 'title' ] print ( title , len ( item . content )) if __name__ == '__main__' : GithubDeveloperSpider . start () This time, there's a line added: concurrency = 5 Here's a brief introduction about concurrency. Some websites are friendly to crawlers, while some are not. If you visit a website too frequently, you will be banned from the server. Besides, to be a good crawler, we should protect the server, rather than making it crash. Not every server can burden a huge spider. To protect both, we have to control our concurrency. Concurrency means the connection numbers in a time. In this case, we set it to 5. Let's have a short look on the log. Output: [2019:01:23 00:01:59]-ruia-INFO spider : Spider started! [2019:01:23 00:01:59]-ruia-WARNINGspider : ruia tried to use loop.add_signal_handler but it is not implemented on this platform. [2019:01:23 00:01:59]-ruia-WARNINGspider : ruia tried to use loop.add_signal_handler but it is not implemented on this platform. [2019:01:23 00:01:59]-Request-INFO request: <GET: https://developer.github.com/v3/> [2019:01:23 00:02:00]-Request-INFO request: <GET: https://developer.github.com/v3/> [2019:01:23 00:02:00]-Request-INFO request: <GET: https://developer.github.com/v3/media/> [2019:01:23 00:02:00]-Request-INFO request: <GET: https://developer.github.com/v3/oauth_authorizations/> [2019:01:23 00:02:00]-Request-INFO request: <GET: https://developer.github.com/v3/auth/> [2019:01:23 00:02:00]-Request-INFO request: <GET: https://developer.github.com/v3/troubleshooting/> [2019:01:23 00:02:01]-Request-INFO request: <GET: https://developer.github.com/v3/previews/> Overview 38490 [2019:01:23 00:02:02]-Request-INFO request: <GET: https://developer.github.com/v3/versions/> OAuth Authorizations API 66565 [2019:01:23 00:02:02]-Request-INFO request: <GET: https://developer.github.com/v3/activity/> Media Types 8652 [2019:01:23 00:02:02]-Request-INFO request: <GET: https://developer.github.com/v3/activity/events/> Troubleshooting 2551 [2019:01:23 00:02:02]-Request-INFO request: <GET: https://developer.github.com/v3/activity/events/types/> API Previews 19537 [2019:01:23 00:02:02]-Request-INFO request: <GET: https://developer.github.com/v3/activity/feeds/> Other Authentication Methods 6651 [2019:01:23 00:02:03]-Request-INFO request: <GET: https://developer.github.com/v3/activity/notifications/> Versions 1344 Feeds 14090 [2019:01:23 00:02:03]-Request-INFO request: <GET: https://developer.github.com/v3/activity/starring/> Activity 2178 [2019:01:23 00:02:04]-Request-INFO request: <GET: https://developer.github.com/v3/activity/watching/> [2019:01:23 00:02:05]-Request-INFO request: <GET: https://developer.github.com/v3/checks/> Events 11844 Starring 55228 [2019:01:23 00:02:05]-Request-INFO request: <GET: https://developer.github.com/v3/checks/runs/> [2019:01:23 00:02:05]-Request-INFO request: <GET: https://developer.github.com/v3/checks/suites/> Event Types & Payloads 1225037 Notifications 65679 Watching 35775 Checks 7379 Check Runs 116607 [2019:01:23 00:02:06]-ruia-INFO spider : Stopping spider: ruia Check Suites 115330 [2019:01:23 00:02:06]-ruia-INFO spider : Total requests: 18 [2019:01:23 00:02:06]-ruia-INFO spider : Time usage: 0:00:07.342048 [2019:01:23 00:02:06]-ruia-INFO spider : Spider finished! Focus on the first several lines. [2019:01:23 00:01:54]-Request-INFO request: <GET: https://developer.github.com/v3/> [2019:01:23 00:02:00]-Request-INFO request: <GET: https://developer.github.com/v3/> [2019:01:23 00:02:00]-Request-INFO request: <GET: https://developer.github.com/v3/media/> [2019:01:23 00:02:00]-Request-INFO request: <GET: https://developer.github.com/v3/oauth_authorizations/> [2019:01:23 00:02:00]-Request-INFO request: <GET: https://developer.github.com/v3/auth/> [2019:01:23 00:02:00]-Request-INFO request: <GET: https://developer.github.com/v3/troubleshooting/> [2019:01:23 00:02:05]-Request-INFO request: <GET: https://developer.github.com/v3/previews/> Overview 38490 [2019:01:23 00:02:07]-Request-INFO request: <GET: https://developer.github.com/v3/versions/> OAuth Authorizations API 66565 The first request is at our requesting the catalogue page. Then, our spider send 5 requests at almost same time, at [00:02:00] . 5 seconds later, at [00:02:05] , our spider receives a response, and then sent another request. The response was parsed immediately. 2 seconds later, at [00:02:07] , our spider receives another response, and sent another request. Then, it parsed the response immediately. That is to say, at any time, there are 5 connections between spider and server. That is concurrency control. Hey, notice that our spider sent 5 requests at same time! Thanks to python's asyncio library, we can write asynchronous crawler easier and faster. Coroutines runs faster than multi-threadings.","title":"Concurrency Control"},{"location":"en/tutorials/spider.html#use-middleware","text":"Ruia provides mainly two ways to enhance itself. Firstly let's talk about middlewares. Middlewares are used to process a request before it's sending and to process a response after it's receiving In a word, it is something between your spider and server. Here is a simple middleware named ruia-ua , it is used to automatically add random User-Agent to your requests. Firstly, install ruia-ua . pip install ruia-ua Then, add it to your spider. from ruia import AttrField , TextField , Item , Spider from ruia_ua import middleware class HackerNewsItem ( Item ): target_item = TextField ( css_select = 'tr.athing' ) title = TextField ( css_select = 'a.storylink' ) url = AttrField ( css_select = 'a.storylink' , attr = 'href' ) async def clean_title ( self , value ): return value class HackerNewsSpider ( Spider ): start_urls = [ 'https://news.ycombinator.com/news?p=1' , 'https://news.ycombinator.com/news?p=2' ] concurrency = 10 async def parse ( self , res ): async for item in HackerNewsItem . get_items ( html = res . html ): print ( item . title ) if __name__ == '__main__' : HackerNewsSpider . start ( middleware = middleware ) ruia.Spider has an argument middleware . It receives a list or a single middleware.","title":"Use Middleware"},{"location":"en/tutorials/spider.html#use-plugin","text":"If you want better control of your spider, try to use some plugins. ruia-pyppeteer is a ruia plugin used for loading JavaScript. Firstly, install ruia-pyppeteer . pip install ruia_pyppeteer # New features pip install git+https://github.com/ruia-plugins/ruia-pyppeteer Note When you use load_js, it will download a recent version of Chromium (~100MB). This only happens once. Here is a simple example to show how to load JavaScript. import asyncio from ruia_pyppeteer import PyppeteerRequest as Request request = Request ( \"https://www.jianshu.com/\" , load_js = True ) response = asyncio . get_event_loop () . run_until_complete ( request . fetch ()) print ( response . html ) Here is an example to use it in your spider: from ruia import AttrField , TextField , Item from ruia_pyppeteer import PyppeteerSpider as Spider from ruia_pyppeteer import PyppeteerRequest as Request class JianshuItem ( Item ): target_item = TextField ( css_select = 'ul.list>li' ) author_name = TextField ( css_select = 'a.name' ) author_url = AttrField ( attr = 'href' , css_select = 'a.name' ) async def clean_author_url ( self , author_url ): return f \"https://www.jianshu.com{author_url}\" class JianshuSpider ( Spider ): start_urls = [ 'https://www.jianshu.com/' ] concurrency = 10 # Load js on the first request load_js = True async def parse ( self , response ): async for item in JianshuItem . get_items ( html = response . html ): # Loading js by using PyppeteerRequest yield Request ( url = item . author_url , load_js = self . load_js , callback = self . parse_item ) async def parse_item ( self , response ): print ( response ) if __name__ == '__main__' : JianshuSpider . start ()","title":"Use Plugin"}]}